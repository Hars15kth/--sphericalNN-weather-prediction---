{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a35be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1+cu128\n",
      "CUDA version: 12.8\n",
      "Supported archs: ['sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Supported archs:\", torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e80597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 09:45:10,510 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-09-15 09:45:10,942 INFO Request ID is 41e5a134-d9b6-4c3a-b85a-7cc62d30e7a9\n",
      "2025-09-15 09:45:11,142 INFO status has been updated to accepted\n",
      "2025-09-15 09:45:16,585 INFO status has been updated to running\n",
      "2025-09-15 09:45:20,170 INFO status has been updated to successful\n",
      "                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:/Users/hars/Documents/era5_data/era5_500hpa_2024_09.nc'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cdsapi\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "c.retrieve(\n",
    "    \"reanalysis-era5-pressure-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"format\": \"netcdf\",\n",
    "        \"variable\": [\n",
    "            \"temperature\", \"geopotential\", \"u_component_of_wind\", \"v_component_of_wind\"\n",
    "        ],\n",
    "        \"pressure_level\": [\"500\"],\n",
    "        \"year\": \"2024\",\n",
    "        \"month\": \"09\",\n",
    "        \"day\": [f\"{d:02d}\" for d in range(1, 31 + 1)],\n",
    "        \"time\": [f\"{h:02d}:00\" for h in range(24)],\n",
    "        \"area\": [90, -180, -90, 180],\n",
    "    },\n",
    "    \"C:/Users/hars/Documents/era5_data/era5_500hpa_2024_09.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22087dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 10:25:43,790 INFO [2025-09-03T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-09-18 10:25:43,791 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-09-18 10:25:44,229 INFO Request ID is 52fb7b30-3da4-4bb7-a2c5-7f0ca991dd46\n",
      "2025-09-18 10:25:44,411 INFO status has been updated to accepted\n",
      "2025-09-18 10:26:18,089 INFO status has been updated to successful\n",
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to D:. Size (MB): 4503.66\n"
     ]
    }
   ],
   "source": [
    "import cdsapi\n",
    "import shutil, os\n",
    "\n",
    "# Step 1: Download ERA5 data to a temp location\n",
    "c = cdsapi.Client()\n",
    "\n",
    "tmp_dir = r\"C:\\Users\\hars\\AppData\\Local\\Temp\"\n",
    "tmp_nc = os.path.join(tmp_dir, \"era5_500hpa_2024_09.tmp\")\n",
    "\n",
    "c.retrieve(\n",
    "    \"reanalysis-era5-pressure-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"format\": \"netcdf\",\n",
    "        \"variable\": [\n",
    "            \"temperature\", \"geopotential\", \"u_component_of_wind\", \"v_component_of_wind\"\n",
    "        ],\n",
    "        \"pressure_level\": [\"500\"],\n",
    "        \"year\": \"2024\",\n",
    "        \"month\": \"09\",\n",
    "        \"day\": [f\"{d:02d}\" for d in range(1, 31 + 1)],\n",
    "        \"time\": [f\"{h:02d}:00\" for h in range(24)],\n",
    "        \"area\": [90, -180, -90, 180],\n",
    "    },\n",
    "    tmp_nc\n",
    ")\n",
    "\n",
    "# Step 2: Atomic copy to D:\\era5_cache\n",
    "dst_dir = r\"D:\\era5_cache\"\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "dst_final = os.path.join(dst_dir, \"era5_500hpa_2024_09.nc\")\n",
    "shutil.copy2(tmp_nc, dst_final)\n",
    "\n",
    "print(\"Copied to D:. Size (MB):\", round(os.path.getsize(dst_final)/(1024*1024), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce37f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Paths and basic config\n",
    "DATA_PATH = \"D:/era5_cache/era5_500hpa_2024_09.nc\"  # your ERA5 file\n",
    "RESULTS_ROOT = \"D:/era5_spherical_resume_results\"   # where previous run artifacts were saved\n",
    "# If you know the exact timestamp folder you want to package, set it here (e.g., \"20250918_153012\").\n",
    "# Otherwise we'll auto-pick the latest in a later cell.\n",
    "SPECIFIC_TIMESTAMP = None  # or e.g. \"20250918_153012\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a05355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports, device, seeds, deterministic/cudnn\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# e3nn primitives (you already installed and imported in earlier cells)\n",
    "from e3nn.o3 import Irreps\n",
    "from e3nn.o3 import FullyConnectedTensorProduct\n",
    "\n",
    "# Repro + speed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cudnn.benchmark = True   # fixed-size kernels accelerate\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223c7369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp/Cache dirs set to: D:\\abacus_temp\n"
     ]
    }
   ],
   "source": [
    "# Cell 2a: Force temp/cache dirs to D:\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "D_DRIVE_ROOT = Path(r\"D:/abacus_temp\")\n",
    "D_DRIVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"TMPDIR\"]  = str(D_DRIVE_ROOT)\n",
    "os.environ[\"TEMP\"]    = str(D_DRIVE_ROOT)\n",
    "os.environ[\"TMP\"]     = str(D_DRIVE_ROOT)\n",
    "os.environ[\"XDG_CACHE_HOME\"] = str(D_DRIVE_ROOT)\n",
    "os.environ[\"TORCH_HOME\"] = str(D_DRIVE_ROOT / \"torch\")\n",
    "os.environ[\"HF_HOME\"]    = str(D_DRIVE_ROOT / \"hf\")\n",
    "\n",
    "print(\"Temp/Cache dirs set to:\", D_DRIVE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c800bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: C:\\Users\\hars\\Documents\\era5_data\\era5_500hpa_2024_09.nc | Exists? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "DATA_PATH = Path(r\"C:/Users/hars/Documents/era5_data/era5_500hpa_2024_09.nc\")\n",
    "print(\"Using:\", DATA_PATH, \"| Exists?\", DATA_PATH.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac3fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(r\"D:/era5_cache/era5_500hpa_2024_09.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f608f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ERA5 file: D:\\era5_cache\\era5_500hpa_2024_09.nc\n",
      "File size (bytes): 4722431171\n",
      "Opened with netcdf4 engine.\n",
      "Selected single pressure level.\n",
      "Opened Dataset summary:\n",
      " - dims: {'time': 720, 'latitude': 721, 'longitude': 1440}\n",
      " - vars: ['t', 'z', 'u', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2b: Robust open of ERA5 NetCDF on Windows\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# Set your data path\n",
    "DATA_PATH = r\"D:\\era5_cache\\era5_500hpa_2024_09.nc\"\n",
    "\n",
    "p = Path(DATA_PATH)\n",
    "print(\"Using ERA5 file:\", p)\n",
    "\n",
    "# Basic checks\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {p}\")\n",
    "size_bytes = p.stat().st_size\n",
    "print(\"File size (bytes):\", size_bytes)\n",
    "if size_bytes < 1024:  # arbitrarily treat <1KB as empty/corrupt\n",
    "    raise ValueError(f\"File seems empty or corrupt (size={size_bytes} bytes).\")\n",
    "\n",
    "# Try netcdf4 first\n",
    "try:\n",
    "    ds = xr.open_dataset(str(p), engine=\"netcdf4\")\n",
    "    print(\"Opened with netcdf4 engine.\")\n",
    "except Exception as e1:\n",
    "    print(\"netcdf4 engine failed:\", repr(e1))\n",
    "    # Try h5netcdf engine for NetCDF4/HDF5 files if available\n",
    "    try:\n",
    "        ds = xr.open_dataset(str(p), engine=\"h5netcdf\")\n",
    "        print(\"Opened with h5netcdf engine.\")\n",
    "    except Exception as e2:\n",
    "        print(\"h5netcdf engine failed:\", repr(e2))\n",
    "        # Try scipy only for classic NetCDF3\n",
    "        try:\n",
    "            ds = xr.open_dataset(str(p), engine=\"scipy\")\n",
    "            print(\"Opened with scipy engine (NetCDF3).\")\n",
    "        except Exception as e3:\n",
    "            print(\"scipy engine failed:\", repr(e3))\n",
    "            raise RuntimeError(\n",
    "                \"Could not open the NetCDF file with netcdf4, h5netcdf, or scipy. \"\n",
    "                \"Check that the file is a valid NetCDF4/HDF5 file and not locked/corrupted.\"\n",
    "            )\n",
    "\n",
    "# Select 500 hPa if present\n",
    "if \"pressure_level\" in ds.dims or \"pressure_level\" in ds.coords:\n",
    "    try:\n",
    "        if 500.0 in ds[\"pressure_level\"].values:\n",
    "            ds = ds.sel(pressure_level=500.0)\n",
    "        else:\n",
    "            ds = ds.isel(pressure_level=0)\n",
    "        print(\"Selected single pressure level.\")\n",
    "    except Exception as e:\n",
    "        print(\"Pressure level selection warning:\", e)\n",
    "\n",
    "# Normalize time coordinate name if needed\n",
    "if \"valid_time\" in ds.dims or \"valid_time\" in ds.coords:\n",
    "    ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "# Print a quick summary\n",
    "print(\"Opened Dataset summary:\")\n",
    "print(\" - dims:\", dict(ds.sizes))\n",
    "print(\" - vars:\", list(ds.data_vars)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfb6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming stats over time=720, latitude=721, longitude=1440\n",
      "Processed time slice [0:16)\n",
      "Processed time slice [16:32)\n",
      "Processed time slice [32:48)\n",
      "Processed time slice [48:64)\n",
      "Processed time slice [64:80)\n",
      "Processed time slice [80:96)\n",
      "Processed time slice [96:112)\n",
      "Processed time slice [112:128)\n",
      "Processed time slice [128:144)\n",
      "Processed time slice [144:160)\n",
      "Processed time slice [160:176)\n",
      "Processed time slice [176:192)\n",
      "Processed time slice [192:208)\n",
      "Processed time slice [208:224)\n",
      "Processed time slice [224:240)\n",
      "Processed time slice [240:256)\n",
      "Processed time slice [256:272)\n",
      "Processed time slice [272:288)\n",
      "Processed time slice [288:304)\n",
      "Processed time slice [304:320)\n",
      "Processed time slice [320:336)\n",
      "Processed time slice [336:352)\n",
      "Processed time slice [352:368)\n",
      "Processed time slice [368:384)\n",
      "Processed time slice [384:400)\n",
      "Processed time slice [400:416)\n",
      "Processed time slice [416:432)\n",
      "Processed time slice [432:448)\n",
      "Processed time slice [448:464)\n",
      "Processed time slice [464:480)\n",
      "Processed time slice [480:496)\n",
      "Processed time slice [496:512)\n",
      "Processed time slice [512:528)\n",
      "Processed time slice [528:544)\n",
      "Processed time slice [544:560)\n",
      "Processed time slice [560:576)\n",
      "Processed time slice [576:592)\n",
      "Processed time slice [592:608)\n",
      "Processed time slice [608:624)\n",
      "Processed time slice [624:640)\n",
      "Processed time slice [640:656)\n",
      "Processed time slice [656:672)\n",
      "Processed time slice [672:688)\n",
      "Processed time slice [688:704)\n",
      "Processed time slice [704:720)\n",
      "Canonical variable order: ['temperature', 'geopotential', 'u', 'v']\n",
      "Stats:\n",
      " - temperature  : mean=254.5019, std=13.3819\n",
      " - geopotential : mean=54447.4493, std=3517.3992\n",
      " - u            : mean=6.1559, std=11.7268\n",
      " - v            : mean=-0.0303, std=8.4741\n",
      "Saved stats to: D:\\era5_cache\\stats_500hpa_2024_09.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 2c: Streaming stats over time (robust dims, low memory)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "assert 'DATA_PATH' in globals(), \"Set DATA_PATH before running Cell 2c.\"\n",
    "\n",
    "# Map dataset variables to canonical names\n",
    "VAR_MAP = {\n",
    "    \"t\": \"temperature\",\n",
    "    \"z\": \"geopotential\",\n",
    "    \"u\": \"u\",\n",
    "    \"v\": \"v\",\n",
    "    # fallbacks\n",
    "    \"ta\": \"temperature\",\n",
    "    \"zg\": \"geopotential\",\n",
    "    \"ua\": \"u\",\n",
    "    \"va\": \"v\",\n",
    "}\n",
    "required = [\"temperature\", \"geopotential\", \"u\", \"v\"]\n",
    "\n",
    "# Helper: ensure we have a proper time dimension called \"time\"\n",
    "def ensure_time_dim(ds_in: xr.Dataset) -> xr.Dataset:\n",
    "    ds_out = ds_in\n",
    "    # Normalize potential alternative names\n",
    "    if \"valid_time\" in ds_out.dims or \"valid_time\" in ds_out.coords:\n",
    "        ds_out = ds_out.rename({\"valid_time\": \"time\"})\n",
    "    # If time exists as a coord but not a dim, try to move it to a dim\n",
    "    if \"time\" not in ds_out.dims:\n",
    "        if \"time\" in ds_out.coords:\n",
    "            # If variables lack time dimension, add it with length 1 (unlikely for ERA5)\n",
    "            needs_expand = []\n",
    "            for v in ds_out.data_vars:\n",
    "                if \"time\" not in ds_out[v].dims:\n",
    "                    needs_expand.append(v)\n",
    "            if needs_expand:\n",
    "                # Expand each variable to include a time=1 dimension\n",
    "                ds_out = ds_out.expand_dims(time=[ds_out[\"time\"].values]).copy()\n",
    "        else:\n",
    "            # No time at all: create a synthetic single time index\n",
    "            ds_out = ds_out.assign_coords(time=np.array([0])).expand_dims(\"time\")\n",
    "    # Sort by time if present\n",
    "    if \"time\" in ds_out.coords:\n",
    "        ds_out = ds_out.sortby(\"time\")\n",
    "    return ds_out\n",
    "\n",
    "# Open lazily with explicit engine\n",
    "ds = xr.open_dataset(DATA_PATH, engine=\"netcdf4\")\n",
    "\n",
    "# Select 500 hPa level (lazy)\n",
    "if \"pressure_level\" in ds.dims or \"pressure_level\" in ds.coords:\n",
    "    if 500.0 in ds[\"pressure_level\"].values:\n",
    "        ds_500 = ds.sel(pressure_level=500.0)\n",
    "    else:\n",
    "        ds_500 = ds.isel(pressure_level=0)\n",
    "else:\n",
    "    ds_500 = ds\n",
    "\n",
    "# Ensure time dimension exists and is named 'time'\n",
    "ds_500 = ensure_time_dim(ds_500)\n",
    "\n",
    "# Build mapping raw->canonical present in ds\n",
    "present = set(ds_500.data_vars)\n",
    "mapped = {}\n",
    "for raw, canon in VAR_MAP.items():\n",
    "    if raw in present and canon not in mapped.values():\n",
    "        mapped[raw] = canon\n",
    "\n",
    "if set(mapped.values()) != set(required):\n",
    "    missing = [v for v in required if v not in mapped.values()]\n",
    "    raise KeyError(f\"Missing required variables: {missing}. Dataset has: {list(ds_500.data_vars)}. \"\n",
    "                   f\"Found mapping: {mapped}\")\n",
    "\n",
    "# Determine dims robustly\n",
    "def first_available(name_options, sizes):\n",
    "    for n in name_options:\n",
    "        if n in sizes:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "time_dim = first_available([\"time\"], ds_500.sizes)\n",
    "lat_dim = first_available([\"latitude\", \"lat\"], ds_500.sizes)\n",
    "lon_dim = first_available([\"longitude\", \"lon\"], ds_500.sizes)\n",
    "\n",
    "if time_dim is None or lat_dim is None or lon_dim is None:\n",
    "    raise KeyError(f\"Could not find required dims. sizes={ds_500.sizes}\")\n",
    "\n",
    "T = ds_500.sizes[time_dim]\n",
    "H = ds_500.sizes[lat_dim]\n",
    "W = ds_500.sizes[lon_dim]\n",
    "print(f\"Streaming stats over {time_dim}={T}, {lat_dim}={H}, {lon_dim}={W}\")\n",
    "\n",
    "# Streaming accumulators per canonical variable\n",
    "acc_sum = {c: 0.0 for c in required}\n",
    "acc_sum2 = {c: 0.0 for c in required}\n",
    "acc_count = {c: 0 for c in required}\n",
    "\n",
    "# Chunk over time to keep memory low\n",
    "chunk_t = 16  # adjust if needed\n",
    "for t0 in range(0, T, chunk_t):\n",
    "    t1 = min(T, t0 + chunk_t)\n",
    "    sl = slice(t0, t1)\n",
    "\n",
    "    for canon in required:\n",
    "        raw = [k for k, v in mapped.items() if v == canon][0]\n",
    "        # Reorder dims to (time, lat, lon), dropping any singleton leftover\n",
    "        da = ds_500[raw].isel({time_dim: sl}).transpose(time_dim, lat_dim, lon_dim)\n",
    "\n",
    "        # Read only this small slice\n",
    "        arr = da.values  # shape ~ [chunk_t, H, W] as float32\n",
    "        # Convert to float64 for stable sums, but no copy if already float64\n",
    "        arr = arr.astype(np.float64, copy=False)\n",
    "\n",
    "        n = arr.size\n",
    "        s = float(arr.sum())\n",
    "        s2 = float((arr * arr).sum())\n",
    "\n",
    "        acc_sum[canon] += s\n",
    "        acc_sum2[canon] += s2\n",
    "        acc_count[canon] += n\n",
    "\n",
    "    print(f\"Processed {time_dim} slice [{t0}:{t1})\")\n",
    "\n",
    "# Compute mean/std\n",
    "stats = {}\n",
    "for canon in required:\n",
    "    n = acc_count[canon]\n",
    "    mean = acc_sum[canon] / max(n, 1)\n",
    "    var = max(acc_sum2[canon] / max(n, 1) - mean * mean, 0.0)\n",
    "    std = float(np.sqrt(var) + 1e-6)\n",
    "    stats[canon] = (float(mean), std)\n",
    "\n",
    "VARS = required\n",
    "\n",
    "print(\"Canonical variable order:\", VARS)\n",
    "print(\"Stats:\")\n",
    "for k in VARS:\n",
    "    m, s = stats[k]\n",
    "    print(f\" - {k:13s}: mean={m:.4f}, std={s:.4f}\")\n",
    "\n",
    "# Optional: save stats to D:\n",
    "try:\n",
    "    out_dir = Path(r\"D:/era5_cache\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / \"stats_500hpa_2024_09.json\"\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump({\"VARS\": VARS, \"stats\": stats}, f, indent=2)\n",
    "    print(\"Saved stats to:\", out_file)\n",
    "except Exception as e:\n",
    "    print(\"Could not save stats to D:, continuing without saving. Reason:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a139ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial verts: {'H': 64, 'W': 128, 'N': 8192}\n",
      "KNN shape: (8192, 16)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 (updated): Spherical vertices + robust chunked KNN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def build_spherical_vertices(H=64, W=128):\n",
    "    lats = np.linspace(-90.0, 90.0, H, endpoint=True)\n",
    "    lons = np.linspace(0.0, 360.0, W, endpoint=False)\n",
    "    lat_grid, lon_grid = np.meshgrid(lats, lons, indexing=\"ij\")\n",
    "    lat_rad = np.deg2rad(lat_grid)\n",
    "    lon_rad = np.deg2rad(lon_grid)\n",
    "\n",
    "    x = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "    y = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "    z = np.sin(lat_rad)\n",
    "    xyz = np.stack([x, y, z], axis=-1).reshape(-1, 3).astype(np.float32)  # [N,3]\n",
    "    latlon = np.stack([lat_grid, lon_grid], axis=-1).reshape(-1, 2).astype(np.float32)  # [N,2]\n",
    "    idx_flat = np.arange(H * W, dtype=np.int64)\n",
    "    return {\"xyz\": xyz, \"latlon\": latlon, \"idx_flat\": idx_flat, \"H\": H, \"W\": W}\n",
    "\n",
    "def knn_on_sphere_chunked(xyz_np: np.ndarray, K: int = 16, block: int = 2048) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Memory-safe KNN by processing query rows in blocks.\n",
    "    Avoids forming the full N x N distance matrix.\n",
    "    \"\"\"\n",
    "    xyz = torch.from_numpy(xyz_np)  # [N,3], CPU\n",
    "    N = xyz.shape[0]\n",
    "    K = int(min(K, max(1, N - 1)))  # clamp\n",
    "    knn_idx = torch.empty(N, K, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, N, block):\n",
    "            i1 = min(N, i0 + block)\n",
    "            # Compute distances for the current block to all points\n",
    "            d = torch.cdist(xyz[i0:i1], xyz, p=2)  # [B,N]\n",
    "            # Make self-distance large\n",
    "            ar = torch.arange(i0, i1)\n",
    "            d[torch.arange(i1 - i0), ar] = 1e9\n",
    "            topk = torch.topk(-d, k=K, dim=1).indices  # nearest -> largest negative\n",
    "            knn_idx[i0:i1] = topk.cpu()\n",
    "    return knn_idx.numpy().astype(np.int64)\n",
    "\n",
    "# Initialize small default verts; weâ€™ll rebuild in Cell 3b\n",
    "verts = build_spherical_vertices(H=64, W=128)\n",
    "# Use chunked KNN to be safe\n",
    "verts[\"knn_idx\"] = knn_on_sphere_chunked(verts[\"xyz\"], K=16, block=1024)\n",
    "print(\"Initial verts:\", {\"H\": verts[\"H\"], \"W\": verts[\"W\"], \"N\": verts[\"H\"] * verts[\"W\"]})\n",
    "print(\"KNN shape:\", verts[\"knn_idx\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c59fdef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: D:\\era5_cache\\runtime\\baseline_ctx_src_64x128.tmpmm\n",
      "Deleted: D:\\era5_cache\\runtime\\baseline_tgt_64x128.tmpmm\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Clean up cache files before rebuilding\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = Path(r\"D:/era5_cache/runtime\")\n",
    "for name in [\n",
    "    \"baseline_ctx_src_64x128.npy\",\n",
    "    \"baseline_tgt_64x128.npy\",\n",
    "    \"spherical_ctx_src_TCN.npy\",\n",
    "    \"spherical_tgt_CN.npy\",\n",
    "    \"baseline_ctx_src_64x128.tmpmm\",\n",
    "    \"baseline_tgt_64x128.tmpmm\",\n",
    "    \"spherical_ctx_src_TCN.tmpmm\",\n",
    "    \"spherical_tgt_CN.tmpmm\",\n",
    "    \"baseline_ctx_src_64x128.npy.tmp\",\n",
    "    \"baseline_tgt_64x128.npy.tmp\",\n",
    "    \"spherical_ctx_src_TCN.npy.tmp\",\n",
    "    \"spherical_tgt_CN.npy.tmp\",\n",
    "]:\n",
    "    p = CACHE_DIR / name\n",
    "    if p.exists():\n",
    "        try:\n",
    "            p.unlink()\n",
    "            print(\"Deleted:\", p)\n",
    "        except Exception as e:\n",
    "            print(\"Could not delete\", p, \"->\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1d194a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_num_threads(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, os\u001b[38;5;241m.\u001b[39mcpu_count() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m4\u001b[39m)))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Ascending latitude view\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m lats \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lats[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m lats[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\hars\\anaconda3\\envs\\sphericalnn\\lib\\site-packages\\xarray\\core\\utils.py:452\u001b[0m, in \u001b[0;36mFrozen.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: K) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m V:\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time'"
     ]
    }
   ],
   "source": [
    "# Cell 3.5 (Windows-robust): Rebuild caches and save directly with np.save\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "CACHE_DIR = Path(r\"D:/era5_cache/runtime\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASELINE_CACHE = CACHE_DIR / \"baseline_ctx_src_64x128.npy\"\n",
    "TARGET_CACHE   = CACHE_DIR / \"baseline_tgt_64x128.npy\"\n",
    "SPHERICAL_CACHE = CACHE_DIR / \"spherical_ctx_src_TCN.npy\"\n",
    "SPHERICAL_TGT   = CACHE_DIR / \"spherical_tgt_CN.npy\"\n",
    "\n",
    "USE_TORCH_RESIZE = False\n",
    "CHUNK = 48\n",
    "torch.set_num_threads(min(8, max(1, os.cpu_count() or 4)))\n",
    "\n",
    "# Ascending latitude view\n",
    "T = ds.sizes[\"time\"]\n",
    "lats = ds[\"latitude\"].values\n",
    "if lats[0] > lats[-1]:\n",
    "    ds_cache = ds.isel(latitude=slice(None, None, -1))\n",
    "    lats = ds_cache[\"latitude\"].values\n",
    "else:\n",
    "    ds_cache = ds\n",
    "\n",
    "VARS = [\"temperature\", \"geopotential\", \"u_component_of_wind\", \"v_component_of_wind\"]\n",
    "C = len(VARS)\n",
    "\n",
    "# Baseline caches: build full array in chunks, then save once\n",
    "if not (BASELINE_CACHE.exists() and TARGET_CACHE.exists()):\n",
    "    print(\"Rebuilding baseline 64x128 caches...\")\n",
    "    Xb = np.empty((T, C, 64, 128), dtype=np.float32)\n",
    "    Yb = np.empty((T, C, 64, 128), dtype=np.float32)\n",
    "\n",
    "    for t0 in range(0, T, CHUNK):\n",
    "        t1 = min(t0 + CHUNK, T)\n",
    "        chunk_list = []\n",
    "        for v in VARS:\n",
    "            arr = ds_cache[v].isel(time=slice(t0, t1)).values.astype(np.float32)  # [Tc, H, W]\n",
    "            chunk_list.append(arr[:, None, ...])  # [Tc,1,H,W]\n",
    "        block = np.concatenate(chunk_list, axis=1)  # [Tc,C,H,W]\n",
    "\n",
    "        if USE_TORCH_RESIZE:\n",
    "            ten = torch.from_numpy(block)  # CPU\n",
    "            ten_rs = F.interpolate(ten, size=(64, 128), mode=\"bilinear\", align_corners=False).numpy()\n",
    "        else:\n",
    "            Tc, Cc, H0, W0 = block.shape\n",
    "            step_h = max(1, H0 // 64)\n",
    "            step_w = max(1, W0 // 128)\n",
    "            ten_rs = block[:, :, ::step_h, ::step_w]\n",
    "            ten_rs = ten_rs[:, :, :64, :128]\n",
    "            if ten_rs.shape[2] < 64 or ten_rs.shape[3] < 128:\n",
    "                pad_h = 64 - ten_rs.shape[2]\n",
    "                pad_w = 128 - ten_rs.shape[3]\n",
    "                ten_rs = np.pad(ten_rs, ((0,0),(0,0),(0,pad_h),(0,pad_w)), mode='edge')\n",
    "\n",
    "        Xb[t0:t1] = ten_rs\n",
    "        Yb[t0:t1] = ten_rs\n",
    "\n",
    "        # Progress\n",
    "        if (t0 // CHUNK) % 4 == 0:\n",
    "            print(f\"  baseline cached t={t0}/{T}\")\n",
    "\n",
    "    # Save directly\n",
    "    np.save(BASELINE_CACHE, Xb)\n",
    "    np.save(TARGET_CACHE,   Yb)\n",
    "    print(\"Baseline caches saved:\", BASELINE_CACHE, TARGET_CACHE)\n",
    "else:\n",
    "    print(\"Baseline caches already present.\")\n",
    "\n",
    "# Spherical caches: build and save once\n",
    "if not (SPHERICAL_CACHE.exists() and SPHERICAL_TGT.exists()):\n",
    "    print(\"Rebuilding spherical vertex caches...\")\n",
    "    if 'verts' not in globals():\n",
    "        raise RuntimeError(\"verts not found. Run Cell 3 (spherical vertices) before this cell.\")\n",
    "    latlon_targets = verts[\"latlon\"]\n",
    "    lons = ds_cache[\"longitude\"].values\n",
    "    lat_idx = np.searchsorted(lats, latlon_targets[:, 0], side=\"left\")\n",
    "    lat_idx = np.clip(lat_idx, 0, len(lats) - 1)\n",
    "    lon_targets = ((latlon_targets[:, 1] - lons[0] + 360) % 360) + lons[0]\n",
    "    lon_idx = np.searchsorted(lons, lon_targets, side=\"left\")\n",
    "    lon_idx = np.clip(lon_idx, 0, len(lons) - 1)\n",
    "    N = lat_idx.shape[0]\n",
    "\n",
    "    Xs = np.empty((T, C, N), dtype=np.float32)\n",
    "    Ys = np.empty((T, C, N), dtype=np.float32)\n",
    "\n",
    "    for t0 in range(0, T, CHUNK):\n",
    "        t1 = min(t0 + CHUNK, T)\n",
    "        chunk_list = []\n",
    "        for v in VARS:\n",
    "            arr = ds_cache[v].isel(time=slice(t0, t1)).values.astype(np.float32)  # [Tc, H, W]\n",
    "            chunk_list.append(arr[:, None, ...])\n",
    "        block = np.concatenate(chunk_list, axis=1)  # [Tc,C,H,W]\n",
    "        gather = block[:, :, lat_idx, lon_idx]      # [Tc,C,N]\n",
    "        Xs[t0:t1] = gather\n",
    "        Ys[t0:t1] = gather\n",
    "\n",
    "        if (t0 // CHUNK) % 4 == 0:\n",
    "            print(f\"  spherical cached t={t0}/{T}\")\n",
    "\n",
    "    np.save(SPHERICAL_CACHE, Xs)\n",
    "    np.save(SPHERICAL_TGT,   Ys)\n",
    "    print(\"Spherical caches saved:\", SPHERICAL_CACHE, SPHERICAL_TGT)\n",
    "else:\n",
    "    print(\"Spherical caches already present.\")\n",
    "\n",
    "print(\"Cache setup complete (Windows-robust).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6a535ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Winds-only datasets + TinyLRU (no Coriolis extra channel)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "WINDS_ONLY = True  # we focus on u, v only\n",
    "\n",
    "def get_raw_key_for(canon_name, ds):\n",
    "    candidates = {\n",
    "        \"temperature\": [\"t\", \"ta\", \"temperature\"],\n",
    "        \"geopotential\": [\"z\", \"zg\", \"geopotential\"],\n",
    "        \"u\": [\"u\", \"ua\"],\n",
    "        \"v\": [\"v\", \"va\"],\n",
    "    }\n",
    "    for raw in candidates[canon_name]:\n",
    "        if raw in ds.data_vars:\n",
    "            return raw\n",
    "    raise KeyError(f\"Could not find raw key for {canon_name} in dataset vars: {list(ds.data_vars)}\")\n",
    "\n",
    "class TinyLRU:\n",
    "    def __init__(self, capacity=64):\n",
    "        self.cap = capacity\n",
    "        self.d = OrderedDict()\n",
    "    def get(self, k):\n",
    "        if k in self.d:\n",
    "            v = self.d.pop(k); self.d[k] = v; return v\n",
    "        return None\n",
    "    def put(self, k, v):\n",
    "        if k in self.d: self.d.pop(k)\n",
    "        self.d[k] = v\n",
    "        if len(self.d) > self.cap: self.d.popitem(last=False)\n",
    "\n",
    "def build_var_list(ds):\n",
    "    base = [\"u\", \"v\"] if WINDS_ONLY else [\"t\", \"z\", \"u\", \"v\"]\n",
    "    return [v for v in base if get_raw_key_for(v, ds)]\n",
    "\n",
    "class ERA5GridDataset(Dataset):\n",
    "    def __init__(self, ds, stats, VARS, time_indices, T_ctx=2, cache_capacity=64):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.stats = stats\n",
    "        self.VARS = VARS\n",
    "        self.T_ctx = T_ctx\n",
    "\n",
    "        self.time_dim = \"time\" if \"time\" in ds.sizes else list(ds.sizes.keys())[0]\n",
    "        self.lat_dim = \"latitude\" if \"latitude\" in ds.sizes else \"lat\"\n",
    "        self.lon_dim = \"longitude\" if \"longitude\" in ds.sizes else \"lon\"\n",
    "\n",
    "        self.time_indices = time_indices\n",
    "        self.raw_keys = {c: get_raw_key_for(c, ds) for c in VARS}\n",
    "        self.cache = TinyLRU(capacity=cache_capacity)\n",
    "\n",
    "    def _read_grid_norm(self, canon_name, t_index):\n",
    "        key = (canon_name, int(t_index))\n",
    "        cached = self.cache.get(key)\n",
    "        if cached is not None: return cached\n",
    "\n",
    "        raw = self.raw_keys[canon_name]\n",
    "        da = self.ds[raw].isel({self.time_dim: t_index})\n",
    "\n",
    "        for d in list(da.dims):\n",
    "            if d not in (self.lat_dim, self.lon_dim, self.time_dim) and da.sizes.get(d, 0) == 1:\n",
    "                da = da.isel({d: 0})\n",
    "\n",
    "        if self.lat_dim not in da.dims or self.lon_dim not in da.dims:\n",
    "            keep_dims = [d for d in da.dims if d in (self.lat_dim, self.lon_dim)]\n",
    "            if len(keep_dims) != 2:\n",
    "                raise ValueError(f\"{raw} dims after squeeze: {da.dims}\")\n",
    "            da = da.transpose(*keep_dims)\n",
    "            if da.dims != (self.lat_dim, self.lon_dim):\n",
    "                da = da.transpose(self.lat_dim, self.lon_dim)\n",
    "        else:\n",
    "            da = da.transpose(self.lat_dim, self.lon_dim)\n",
    "\n",
    "        arr = torch.from_numpy(da.values.astype(\"float32\"))\n",
    "        mean, std = self.stats[canon_name]\n",
    "        out = (arr - mean) / std\n",
    "        self.cache.put(key, out)\n",
    "        return out\n",
    "\n",
    "    def __len__(self): return len(self.time_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.time_indices[idx]\n",
    "        ctx_indices = list(range(max(0, t - self.T_ctx), t))\n",
    "\n",
    "        x_list = []\n",
    "        for ti in ctx_indices:\n",
    "            c_list = [self._read_grid_norm(c, ti) for c in self.VARS]\n",
    "            x_t = torch.stack(c_list, dim=0)\n",
    "            x_list.append(x_t)\n",
    "        if len(x_list) == 0:\n",
    "            Hds = self.ds.sizes[self.lat_dim]; Wds = self.ds.sizes[self.lon_dim]\n",
    "            x = torch.zeros(self.T_ctx, len(self.VARS), Hds, Wds, dtype=torch.float32)\n",
    "        else:\n",
    "            while len(x_list) < self.T_ctx: x_list.insert(0, x_list[0])\n",
    "            x = torch.stack(x_list, dim=0)\n",
    "\n",
    "        y_list = [self._read_grid_norm(c, t) for c in self.VARS]\n",
    "        y = torch.stack(y_list, dim=0)\n",
    "        return x, y\n",
    "\n",
    "class ERA5SphericalDataset(Dataset):\n",
    "    def __init__(self, ds, stats, VARS, time_indices, verts, T_ctx=2, cache_capacity=64):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.stats = stats\n",
    "        self.VARS = VARS\n",
    "        self.T_ctx = T_ctx\n",
    "        self.verts = verts\n",
    "\n",
    "        self.time_dim = \"time\" if \"time\" in ds.sizes else list(ds.sizes.keys())[0]\n",
    "        self.lat_dim = \"latitude\" if \"latitude\" in ds.sizes else \"lat\"\n",
    "        self.lon_dim = \"longitude\" if \"longitude\" in ds.sizes else \"lon\"\n",
    "\n",
    "        self.time_indices = time_indices\n",
    "        self.raw_keys = {c: get_raw_key_for(c, ds) for c in VARS}\n",
    "\n",
    "        self.H = ds.sizes[self.lat_dim]\n",
    "        self.W = ds.sizes[self.lon_dim]\n",
    "\n",
    "        self.cache = TinyLRU(capacity=cache_capacity)\n",
    "\n",
    "    def _read_grid_norm(self, canon_name, t_index):\n",
    "        key = (canon_name, int(t_index))\n",
    "        cached = self.cache.get(key)\n",
    "        if cached is not None: return cached\n",
    "\n",
    "        raw = self.raw_keys[canon_name]\n",
    "        da = self.ds[raw].isel({self.time_dim: t_index})\n",
    "        for d in list(da.dims):\n",
    "            if d not in (self.lat_dim, self.lon_dim, self.time_dim) and da.sizes.get(d, 0) == 1:\n",
    "                da = da.isel({d: 0})\n",
    "        if self.lat_dim not in da.dims or self.lon_dim not in da.dims:\n",
    "            keep_dims = [d for d in da.dims if d in (self.lat_dim, self.lon_dim)]\n",
    "            if len(keep_dims) != 2:\n",
    "                raise ValueError(f\"{raw} dims after squeeze: {da.dims}\")\n",
    "            da = da.transpose(*keep_dims)\n",
    "            if da.dims != (self.lat_dim, self.lon_dim):\n",
    "                da = da.transpose(self.lat_dim, self.lon_dim)\n",
    "        else:\n",
    "            da = da.transpose(self.lat_dim, self.lon_dim)\n",
    "\n",
    "        arr = torch.from_numpy(da.values.astype(\"float32\"))\n",
    "        mean, std = self.stats[canon_name]\n",
    "        out = (arr - mean) / std\n",
    "        self.cache.put(key, out)\n",
    "        return out\n",
    "\n",
    "    def __len__(self): return len(self.time_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.time_indices[idx]\n",
    "        ctx_indices = list(range(max(0, t - self.T_ctx), t))\n",
    "\n",
    "        x_list = []\n",
    "        for ti in ctx_indices:\n",
    "            c_list = [self._read_grid_norm(c, ti) for c in self.VARS]\n",
    "            x_t = torch.stack(c_list, dim=0)  # [C,H,W] with C=len(VARS)=2\n",
    "            x_list.append(x_t)\n",
    "        if len(x_list) == 0:\n",
    "            x = torch.zeros(self.T_ctx, len(self.VARS), self.H, self.W, dtype=torch.float32)\n",
    "        else:\n",
    "            while len(x_list) < self.T_ctx: x_list.insert(0, x_list[0])\n",
    "            x = torch.stack(x_list, dim=0)\n",
    "\n",
    "        y_list = [self._read_grid_norm(c, t) for c in self.VARS]\n",
    "        y = torch.stack(y_list, dim=0)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "334c93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Equivariant point encoder (Irreps + FCTP), version-agnostic Gate handling\n",
    "def knn_indices(xyz: torch.Tensor, k: int = 16) -> torch.Tensor:\n",
    "    # xyz: [B,N,3] -> idx: [B,N,K]\n",
    "    with torch.no_grad():\n",
    "        d = torch.cdist(xyz, xyz)  # [B,N,N]\n",
    "        vals, idx = torch.topk(d, k=k+1, largest=False)  # includes self\n",
    "        return idx[:, :, 1:]  # drop self\n",
    "\n",
    "class EquivariantPointBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Message passing from relative vectors (1o) and scalar node features (0e).\n",
    "    Uses a tensor product to mix inputs into a hidden representation and then\n",
    "    applies a simple gated nonlinearity constructed explicitly to avoid e3nn Gate API differences.\n",
    "    \"\"\"\n",
    "    def __init__(self, irreps_in: Irreps, irreps_hidden: Irreps, irreps_out: Irreps, scalars_gate: int = 16):\n",
    "        super().__init__()\n",
    "        # Tensor product: (features x 1o) -> hidden\n",
    "        self.tp1 = FullyConnectedTensorProduct(irreps_in, Irreps(\"1x1o\"), irreps_hidden)\n",
    "\n",
    "        # We implement a gate manually:\n",
    "        # - produce gate scalars g in 0e via an MLP\n",
    "        # - apply sigmoid to g to gate the hidden features projected to out\n",
    "        self.lin_hidden = nn.Linear(irreps_hidden.dim, irreps_hidden.dim)\n",
    "        self.lin_out = nn.Linear(irreps_hidden.dim, irreps_out.dim)\n",
    "        self.gate_mlp = nn.Sequential(\n",
    "            nn.Linear(irreps_hidden.dim, scalars_gate),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(scalars_gate, irreps_out.dim),  # gate dimension matches out scalars\n",
    "        )\n",
    "\n",
    "        # Lift scalar inputs\n",
    "        self.lift = nn.Sequential(\n",
    "            nn.Linear(irreps_in.dim, irreps_in.dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(irreps_in.dim, irreps_in.dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, feats_in: torch.Tensor, rel_vec: torch.Tensor):\n",
    "        # feats_in: [B,N,dim(irreps_in)] packed scalars\n",
    "        # rel_vec: [B,N,K,3] (treated as 1o)\n",
    "        B, N, K, _ = rel_vec.shape\n",
    "\n",
    "        x = self.lift(feats_in)                             # [B,N,dim(ir_in)]\n",
    "        x = x.unsqueeze(2).expand(B, N, K, x.shape[-1])     # [B,N,K,dim(ir_in)]\n",
    "        x = x.reshape(B, N*K, -1)                           # [B,NK,dim(ir_in)]\n",
    "        v = rel_vec.reshape(B, N*K, 3)                      # [B,NK,3]\n",
    "\n",
    "        h = self.tp1(x, v)                                  # [B,NK,dim(ir_hidden)]\n",
    "        h = F.silu(self.lin_hidden(h))                      # [B,NK,dim(ir_hidden)]\n",
    "\n",
    "        gates = torch.sigmoid(self.gate_mlp(h))             # [B,NK,dim(ir_out)]\n",
    "        h = self.lin_out(h)                                  # [B,NK,dim(ir_out)]\n",
    "        h = gates * h                                        # gated output\n",
    "\n",
    "        h = h.view(B, N, K, -1).mean(dim=2)                 # [B,N,dim(ir_out)]\n",
    "        return h\n",
    "\n",
    "class E3NNPointEncoder(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_irreps: str = \"16x0e + 16x1o\", out_channels: int = 32, k: int = 16):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.ir_in = Irreps(f\"{in_channels}x0e\")\n",
    "        self.ir_hidden = Irreps(hidden_irreps)\n",
    "        self.ir_out = Irreps(f\"{out_channels}x0e\")\n",
    "        self.lin_in = nn.Linear(in_channels, self.ir_in.dim)\n",
    "        self.block1 = EquivariantPointBlock(self.ir_in, self.ir_hidden, self.ir_out, scalars_gate=16)\n",
    "        self.lin_out = nn.Linear(self.ir_out.dim, out_channels)\n",
    "\n",
    "    def forward(self, feats: torch.Tensor, xyz: torch.Tensor):\n",
    "        # feats: [B, N, Cin] (scalars), xyz: [B, N, 3]\n",
    "        B, N, _ = feats.shape\n",
    "        idx = knn_indices(xyz, k=self.k)  # [B,N,K]\n",
    "        nbrs = torch.gather(xyz, 1, idx.unsqueeze(-1).expand(-1, -1, -1, 3))  # [B,N,K,3]\n",
    "        rel = nbrs - xyz.unsqueeze(2)  # [B,N,K,3]\n",
    "        x = self.lin_in(feats)         # [B,N,dim(ir_in)]\n",
    "        h = self.block1(x, rel)        # [B,N,dim(ir_out)]\n",
    "        h = self.lin_out(h)            # [B,N,Cout] (0e scalars)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fc15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Stronger Spherical model (edge-aware KNN + 2-layer Conv3dLSTM), grid-safe input\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv3dLSTMCell(nn.Module):\n",
    "    def __init__(self, in_ch, hidden_ch, kernel_size=3):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        self.conv = nn.Conv3d(in_ch + hidden_ch, 4 * hidden_ch, kernel_size, padding=pad)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        gates = self.conv(torch.cat([x, h], dim=1))\n",
    "        ci, cf, co, cg = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(ci)\n",
    "        f = torch.sigmoid(cf)\n",
    "        o = torch.sigmoid(co)\n",
    "        g = torch.tanh(cg)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class SphericalSpatioTemporal(nn.Module):\n",
    "    def __init__(self, n_vars, verts, hidden_ch=64, edge_ch=16, T_ctx=4):\n",
    "        super().__init__()\n",
    "        self.C = n_vars\n",
    "        self.T_ctx = T_ctx\n",
    "\n",
    "        xyz = torch.from_numpy(verts[\"xyz\"]).float()        # [N,3]\n",
    "        idx = torch.from_numpy(verts[\"knn_idx\"]).long()     # [N,K]\n",
    "        idx_flat = torch.from_numpy(verts[\"idx_flat\"]).long()\n",
    "        self.H = int(verts[\"H\"])\n",
    "        self.W = int(verts[\"W\"])\n",
    "        self.N = self.H * self.W\n",
    "\n",
    "        self.register_buffer(\"xyz\", xyz)\n",
    "        self.register_buffer(\"idx\", idx)\n",
    "        self.register_buffer(\"idx_flat\", idx_flat)\n",
    "\n",
    "        self.K = idx.shape[1]\n",
    "\n",
    "        self.lin_node = nn.Linear(n_vars, hidden_ch)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(4, edge_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(edge_ch, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.core1 = Conv3dLSTMCell(in_ch=hidden_ch, hidden_ch=hidden_ch, kernel_size=3)\n",
    "        self.core2 = Conv3dLSTMCell(in_ch=hidden_ch, hidden_ch=hidden_ch, kernel_size=3)\n",
    "        self.lin_out = nn.Linear(hidden_ch, n_vars)\n",
    "\n",
    "    def _message_pass(self, feats, device):\n",
    "        # feats: [B, T, N, F]\n",
    "        B, T, N, F = feats.shape\n",
    "        K = self.K\n",
    "        xyz = self.xyz.to(device)  # [N,3]\n",
    "        idx = self.idx.to(device)  # [N,K]\n",
    "\n",
    "        xyz_b = xyz.unsqueeze(0).expand(B, -1, -1)          # [B,N,3]\n",
    "        idx_b = idx.unsqueeze(0).expand(B, -1, -1)          # [B,N,K]\n",
    "        b_ix = torch.arange(B, device=device).view(B,1,1).expand(B, N, K)\n",
    "        nbrs = xyz_b[b_ix, idx_b, :]                        # [B,N,K,3]\n",
    "        rel = nbrs - xyz_b.unsqueeze(2)                     # [B,N,K,3]\n",
    "        dist = torch.linalg.norm(rel, dim=-1, keepdim=True) # [B,N,K,1]\n",
    "        rel_feat = torch.cat([rel, dist], dim=-1)           # [B,N,K,4]\n",
    "        w = self.edge_mlp(rel_feat)                         # [B,N,K,1]\n",
    "\n",
    "        idx_bt = idx_b.unsqueeze(1).expand(B, T, N, K)      # [B,T,N,K]\n",
    "        b_ix_bt = torch.arange(B, device=device).view(B,1,1,1).expand(B, T, N, K)\n",
    "        t_ix_bt = torch.arange(T, device=device).view(1,T,1,1).expand(B, T, N, K)\n",
    "        nbr_feats = feats[b_ix_bt, t_ix_bt, idx_bt, :]      # [B,T,N,K,F]\n",
    "\n",
    "        agg = (w.unsqueeze(1) * nbr_feats).sum(dim=3) / (w.unsqueeze(1).sum(dim=3) + 1e-6)  # [B,T,N,F]\n",
    "        return 0.5 * feats + 0.5 * agg\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Accept either [B, T, C, Hds, Wds] or [B, T, C, N]\n",
    "        B = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        if x.dim() == 5:\n",
    "            # x: [B,T,C,Hds,Wds] -> resample to target verts grid HxW then flatten\n",
    "            _, T, C, Hds, Wds = x.shape\n",
    "            if (Hds, Wds) != (self.H, self.W):\n",
    "                x = x.view(B*T, C, Hds, Wds)\n",
    "                x = F.interpolate(x, size=(self.H, self.W), mode=\"bilinear\", align_corners=False)\n",
    "                x = x.view(B, T, C, self.H, self.W)\n",
    "            x = x.view(B, T, C, -1)  # [B,T,C,N]\n",
    "        else:\n",
    "            # x: [B,T,C,N]\n",
    "            T = x.shape[1]\n",
    "            C = x.shape[2]\n",
    "            assert x.shape[-1] == self.N, f\"N mismatch: got {x.shape[-1]}, expected {self.N}\"\n",
    "\n",
    "        # Node encode: [B,T,N,C] -> [B,T,N,Hid]\n",
    "        feats = torch.tanh(self.lin_node(x.permute(0,1,3,2)))  # [B,T,N,Hid]\n",
    "\n",
    "        # Message passing on sphere\n",
    "        feats = self._message_pass(feats, device)              # [B,T,N,Hid]\n",
    "\n",
    "        # Gridify for Conv3dLSTM: [B,Hid,T,H,W]\n",
    "        hid = feats.permute(0,3,1,2).contiguous()              # [B,Hid,T,N]\n",
    "        grid = torch.zeros(B, hid.shape[1], T, self.H, self.W, device=device, dtype=hid.dtype)\n",
    "        grid.view(B, hid.shape[1], T, -1)[:, :, :, self.idx_flat] = hid\n",
    "        x3d = grid\n",
    "\n",
    "        # Temporal core\n",
    "        h1 = torch.zeros_like(x3d); c1 = torch.zeros_like(x3d)\n",
    "        h1, c1 = self.core1(x3d, h1, c1)\n",
    "        h2 = torch.zeros_like(h1); c2 = torch.zeros_like(h1)\n",
    "        h2, c2 = self.core2(h1, h2, c2)                        # [B,Hid,T,H,W]\n",
    "\n",
    "        h_last = h2[:, :, -1]                                  # [B,Hid,H,W]\n",
    "\n",
    "        # Back to nodes\n",
    "        feat_nodes = h_last.flatten(2)[:, :, self.idx_flat]    # [B,Hid,N]\n",
    "        out_nodes = self.lin_out(feat_nodes.permute(0,2,1))    # [B,N,C]\n",
    "\n",
    "        # To grid [B,C,H,W]\n",
    "        out_grid = torch.zeros(B, self.C, self.H, self.W, device=device, dtype=out_nodes.dtype)\n",
    "        out_grid.view(B, self.C, -1)[:, :, self.idx_flat] = out_nodes.permute(0,2,1)\n",
    "        return out_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678d81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 (grad-safe minimal baseline): no @torch.no_grad, fixed 64x128 internal grid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c_in, c_out, 3, padding=1, bias=True)\n",
    "        self.act = nn.SiLU()\n",
    "        nn.init.kaiming_uniform_(self.conv.weight, a=0.2, nonlinearity='leaky_relu')\n",
    "        if self.conv.bias is not None:\n",
    "            nn.init.zeros_(self.conv.bias)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "class BaselineLatLon(nn.Module):\n",
    "    \"\"\"\n",
    "    Winds-only baseline that:\n",
    "      - downsamples inputs to 64x128,\n",
    "      - runs a tiny CNN,\n",
    "      - upsamples back to match input HxW.\n",
    "    No no_grad used anywhere; fully differentiable.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_vars: int, hidden_ch: int = 16, T_ctx: int = 4, H: int = 64, W: int = 128):\n",
    "        super().__init__()\n",
    "        self.C = n_vars\n",
    "        self.T_ctx = T_ctx\n",
    "        self.H0, self.W0 = 64, 128\n",
    "        in_ch = n_vars * T_ctx\n",
    "        self.enc1 = ConvBlock(in_ch, hidden_ch)\n",
    "        self.enc2 = ConvBlock(hidden_ch, hidden_ch)\n",
    "        self.head = nn.Conv2d(hidden_ch, n_vars, 1, bias=True)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):  # x: [B,T,C,H,W]\n",
    "        assert x.ndim == 5, f\"Expected [B,T,C,H,W], got {tuple(x.shape)}\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        assert T == self.T_ctx and C == self.C\n",
    "\n",
    "        # Downsample to internal grid (differentiable)\n",
    "        if (H, W) != (self.H0, self.W0):\n",
    "            x_small = F.interpolate(\n",
    "                x.reshape(B*T, C, H, W),\n",
    "                size=(self.H0, self.W0),\n",
    "                mode=\"bilinear\", align_corners=False\n",
    "            ).view(B, T, C, self.H0, self.W0)\n",
    "        else:\n",
    "            x_small = x\n",
    "\n",
    "        # CNN at 64x128\n",
    "        x2d = x_small.reshape(B, T*C, self.H0, self.W0)\n",
    "        h = self.enc2(self.enc1(x2d))\n",
    "        y_small = self.head(h)  # [B, C, H0, W0]\n",
    "\n",
    "        # Upsample back to original grid (differentiable)\n",
    "        if (H, W) != (self.H0, self.W0):\n",
    "            y = F.interpolate(y_small, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        else:\n",
    "            y = y_small\n",
    "\n",
    "        # Clamp (still differentiable where in-range; saturates outside)\n",
    "        y = torch.clamp(y, -50, 50)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b28ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train 32 | val 8 | test 8 | VARS=['u', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Splits + loaders (winds-only, small caps)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "VARS = build_var_list(ds)  # should be [\"u\",\"v\"]\n",
    "assert set(VARS) >= {\"u\", \"v\"}, f\"VARS={VARS} must include u,v; got {VARS}\"\n",
    "\n",
    "T_CTX = 2\n",
    "BATCH = 1\n",
    "\n",
    "T_total = ds.sizes[\"time\"] if \"time\" in ds.sizes else list(ds.sizes.values())[0]\n",
    "idx_all = [t for t in range(T_total) if t - T_CTX >= 0]\n",
    "\n",
    "CAP_TRAIN, CAP_VAL, CAP_TEST = 32, 8, 8\n",
    "\n",
    "n_total = len(idx_all)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val   = int(0.1 * n_total)\n",
    "train_idx = idx_all[:n_train][:CAP_TRAIN]\n",
    "val_idx   = idx_all[n_train:n_train+n_val][:CAP_VAL]\n",
    "test_idx  = idx_all[n_train+n_val:][:CAP_TEST]\n",
    "\n",
    "print(f\"Splits -> train {len(train_idx)} | val {len(val_idx)} | test {len(test_idx)} | VARS={VARS}\")\n",
    "\n",
    "train_b = ERA5GridDataset(ds, stats, VARS, time_indices=train_idx, T_ctx=T_CTX, cache_capacity=64)\n",
    "val_b   = ERA5GridDataset(ds, stats, VARS, time_indices=val_idx,   T_ctx=T_CTX, cache_capacity=64)\n",
    "test_b  = ERA5GridDataset(ds, stats, VARS, time_indices=test_idx,  T_ctx=T_CTX, cache_capacity=64)\n",
    "\n",
    "train_s = ERA5SphericalDataset(ds, stats, VARS, time_indices=train_idx, verts=verts, T_ctx=T_CTX, cache_capacity=64)\n",
    "val_s   = ERA5SphericalDataset(ds, stats, VARS, time_indices=val_idx,   verts=verts, T_ctx=T_CTX, cache_capacity=64)\n",
    "test_s  = ERA5SphericalDataset(ds, stats, VARS, time_indices=test_idx,  verts=verts, T_ctx=T_CTX, cache_capacity=64)\n",
    "\n",
    "train_b_loader = DataLoader(train_b, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
    "val_b_loader   = DataLoader(val_b,   batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_b_loader  = DataLoader(test_b,  batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "train_s_loader = DataLoader(train_s, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
    "val_s_loader   = DataLoader(val_s,   batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_s_loader  = DataLoader(test_s,  batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eb7212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: torch.Size([1, 2, 2, 721, 1440]) torch.Size([1, 2, 721, 1440])\n",
      "Spherical: torch.Size([1, 2, 2, 721, 1440]) torch.Size([1, 2, 721, 1440])\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: sanity shapes\n",
    "xb, yb = next(iter(train_b_loader))\n",
    "xs, ys = next(iter(train_s_loader))\n",
    "print(\"Baseline:\", xb.shape, yb.shape)   # [B, T_ctx, C, H, W], [B, C, H, W]\n",
    "print(\"Spherical:\", xs.shape, ys.shape)  # same C=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0041195f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_s_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m H, W\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Detect dataset grid from spherical loader\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m xs_chk, ys_chk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_s_loader\u001b[49m))  \u001b[38;5;66;03m# [B,T,C,Hds,Wds], [B,C,Hds,Wds]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m Hds, Wds \u001b[38;5;241m=\u001b[39m ys_chk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], ys_chk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     21\u001b[0m N_detect \u001b[38;5;241m=\u001b[39m Hds \u001b[38;5;241m*\u001b[39m Wds\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_s_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 3b (updated): Rebuild verts at capped resolution with chunked KNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def infer_hw_from_N(N: int):\n",
    "    candidates = [(64,128), (90,180), (128,256), (64,64), (32,128), (32,64), (45,90)]\n",
    "    for H, W in candidates:\n",
    "        if H * W == N:\n",
    "            return H, W\n",
    "    if N % 128 == 0:\n",
    "        return N // 128, 128\n",
    "    H = int(np.sqrt(N))\n",
    "    while H > 1 and N % H != 0:\n",
    "        H -= 1\n",
    "    W = N // H\n",
    "    return H, W\n",
    "\n",
    "# Detect dataset grid from spherical loader\n",
    "xs_chk, ys_chk = next(iter(test_s_loader))  # [B,T,C,Hds,Wds], [B,C,Hds,Wds]\n",
    "Hds, Wds = ys_chk.shape[-2], ys_chk.shape[-1]\n",
    "N_detect = Hds * Wds\n",
    "H_det, W_det = infer_hw_from_N(N_detect)\n",
    "print(f\"Detected spherical dataset grid: {Hds}x{Wds} (N={N_detect}) -> naive verts HxW={H_det}x{W_det}\")\n",
    "\n",
    "# Cap verts to a safe resolution (adjust if you want denser)\n",
    "H_cap, W_cap = 64, 128   # 64x128 is very safe; you can try 90x180 after this is stable\n",
    "print(f\"Capping verts to HxW={H_cap}x{W_cap} (N={H_cap*W_cap}) for memory-safe KNN...\")\n",
    "\n",
    "verts = build_spherical_vertices(H=H_cap, W=W_cap)\n",
    "verts[\"knn_idx\"] = knn_on_sphere_chunked(verts[\"xyz\"], K=16, block=1024)\n",
    "print(\"Rebuilt verts:\", {\"H\": verts[\"H\"], \"W\": verts[\"W\"], \"N\": verts[\"H\"] * verts[\"W\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ccc8977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | AMP dtype: torch.bfloat16\n",
      "Baseline grid set to: 64x128\n",
      "Params (M): baseline 0.003 | spherical 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hars\\AppData\\Local\\Temp\\ipykernel_31868\\1076296082.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_b = torch.cuda.amp.GradScaler(enabled=use_scaler)\n",
      "C:\\Users\\hars\\AppData\\Local\\Temp\\ipykernel_31868\\1076296082.py:76: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_s = torch.cuda.amp.GradScaler(enabled=use_scaler)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 (updated): robust baseline instantiation + spherical (winds-only) + AMP-safe\n",
    "\n",
    "import torch, inspect\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (device == \"cuda\") and torch.cuda.is_bf16_supported()\n",
    "amp_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"Device:\", device, \"| AMP dtype:\", amp_dtype)\n",
    "\n",
    "C = len(VARS)  # should be 2\n",
    "\n",
    "# Force baseline grid to training/cache grid (avoid inferring 721x1440)\n",
    "H_target, W_target = 64, 128\n",
    "print(f\"Baseline grid set to: {H_target}x{W_target}\")\n",
    "\n",
    "def make_baseline():\n",
    "    sig = inspect.signature(BaselineLatLon.__init__)\n",
    "    params = list(sig.parameters.keys())[1:]\n",
    "    mapped = {}\n",
    "    for p in params:\n",
    "        lp = p.lower()\n",
    "        if lp in (\"c_in\",\"in_channels\",\"channels_in\",\"input_channels\",\"cin\",\"n_vars\",\"num_vars\",\"nvars\"):\n",
    "            mapped[p] = C\n",
    "        elif lp in (\"c_out\",\"out_channels\",\"channels_out\",\"output_channels\",\"cout\"):\n",
    "            mapped[p] = C\n",
    "        elif lp in (\"c_hidden\",\"hidden_channels\",\"mid_channels\",\"width\",\"hidden_dim\",\"hidden_size\",\"hidden_ch\"):\n",
    "            mapped[p] = 16\n",
    "        elif lp in (\"t_ctx\",\"tcontext\",\"context\",\"context_steps\",\"t\"):\n",
    "            mapped[p] = T_CTX\n",
    "        elif lp in (\"h\",\"height\"):\n",
    "            mapped[p] = H_target\n",
    "        elif lp in (\"w\",\"width\"):\n",
    "            mapped[p] = W_target\n",
    "        elif lp in (\"grid_size\",\"shape\",\"hw\"):\n",
    "            mapped[p] = (H_target, W_target)\n",
    "        else:\n",
    "            pass\n",
    "    try:\n",
    "        return BaselineLatLon(**mapped).to(device)\n",
    "    except TypeError as e_kw:\n",
    "        patterns = [\n",
    "            (\"C_in\",\"C_out\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\",\"T_ctx\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\",\"T_ctx\",\"H\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\",\"T_ctx\",\"H\",\"W\"),\n",
    "        ]\n",
    "        vals = {\"C_in\":C, \"C_out\":C, \"hidden\":16, \"T_ctx\":T_CTX, \"H\":H_target, \"W\":W_target}\n",
    "        for pat in patterns:\n",
    "            args = [vals[k] for k in pat]\n",
    "            try:\n",
    "                return BaselineLatLon(*args).to(device)\n",
    "            except TypeError:\n",
    "                continue\n",
    "        print(\"BaselineLatLon signature:\", sig)\n",
    "        raise e_kw\n",
    "\n",
    "baseline = make_baseline()\n",
    "\n",
    "# Spherical model (winds-only). Ensure 'verts' is defined before this cell runs.\n",
    "spherical = SphericalSpatioTemporal(\n",
    "    n_vars=C,\n",
    "    verts=verts,\n",
    "    hidden_ch=48,   # modest capacity\n",
    "    edge_ch=16,\n",
    "    T_ctx=T_CTX\n",
    ").to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_b = torch.optim.AdamW(baseline.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "opt_s = torch.optim.AdamW(spherical.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# AMP scaler: enable only for fp16 (not needed for bf16); we start disabled for stability\n",
    "use_scaler = (device == \"cuda\") and (amp_dtype == torch.float16) and False\n",
    "scaler_b = torch.cuda.amp.GradScaler(enabled=use_scaler)\n",
    "scaler_s = torch.cuda.amp.GradScaler(enabled=use_scaler)\n",
    "\n",
    "# Param counts\n",
    "p_b = sum(p.numel() for p in baseline.parameters())\n",
    "p_s = sum(p.numel() for p in spherical.parameters())\n",
    "print(f\"Params (M): baseline {p_b/1e6:.3f} | spherical {p_s/1e6:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56fc5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hars\\AppData\\Local\\Temp\\ipykernel_31868\\2377318045.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_b = torch.cuda.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
      "C:\\Users\\hars\\AppData\\Local\\Temp\\ipykernel_31868\\2377318045.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_s = torch.cuda.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/24 | dt=0.0s | baseline 2.1518 | spherical 1.4511\n",
      "Step 7/24 | dt=0.2s | baseline 1.0912 | spherical 0.8042\n",
      "Step 13/24 | dt=0.3s | baseline 0.5835 | spherical 0.4227\n",
      "Step 19/24 | dt=13.3s | baseline 1.4003 | spherical 0.3121\n",
      "Step 24/24 | dt=64.3s | baseline 1.4090 | spherical 0.2377\n",
      "Training pass complete (angle+MSE, AMP-safe).\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: angle+MSE training, AMP-safe, inline backward\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (device == \"cuda\") and torch.cuda.is_bf16_supported()\n",
    "amp_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "baseline.train(); spherical.train()\n",
    "for p in baseline.parameters(): p.requires_grad_(True)\n",
    "for p in spherical.parameters(): p.requires_grad_(True)\n",
    "\n",
    "for g in opt_b.param_groups: g[\"lr\"] = min(g[\"lr\"], 1e-3)\n",
    "for g in opt_s.param_groups: g[\"lr\"] = min(g[\"lr\"], 1e-3)\n",
    "\n",
    "steps_cap = 24\n",
    "print_every = 6\n",
    "eps = 1e-6\n",
    "\n",
    "if device == \"cuda\":\n",
    "    ac = torch.amp.autocast(device_type=\"cuda\", dtype=amp_dtype)\n",
    "    scaler_b = torch.cuda.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
    "    scaler_s = torch.cuda.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
    "else:\n",
    "    ac = nullcontext()\n",
    "    scaler_b = torch.cuda.amp.GradScaler(enabled=False)\n",
    "    scaler_s = torch.cuda.amp.GradScaler(enabled=False)\n",
    "\n",
    "def angle_loss(pred_uv, true_uv, eps=1e-6, mask=None):\n",
    "    pu, pv = pred_uv[:, 0], pred_uv[:, 1]\n",
    "    tu, tv = true_uv[:, 0], true_uv[:, 1]\n",
    "    pn = torch.sqrt(pu * pu + pv * pv + eps)\n",
    "    tn = torch.sqrt(tu * tu + tv * tv + eps)\n",
    "    cos = (pu * tu + pv * tv) / (pn * tn + eps)\n",
    "    cos = torch.clamp(cos, -1.0, 1.0)\n",
    "    ang = torch.acos(cos)\n",
    "    if mask is not None:\n",
    "        ang = (ang * mask).sum() / (mask.sum() + eps)\n",
    "    else:\n",
    "        ang = ang.mean()\n",
    "    return ang\n",
    "\n",
    "def speed_mask(true_uv, thresh=0.15):\n",
    "    sp = torch.sqrt(true_uv[:,0]**2 + true_uv[:,1]**2)\n",
    "    return (sp >= thresh).float()\n",
    "\n",
    "def finite_or_zero(t):\n",
    "    return torch.where(torch.isfinite(t), t, torch.zeros_like(t))\n",
    "\n",
    "it_b = iter(train_b_loader)\n",
    "it_s = iter(train_s_loader)\n",
    "t0 = time.time()\n",
    "\n",
    "w_mse, w_ang = 1.0, 0.1\n",
    "\n",
    "for step in range(steps_cap):\n",
    "    try: xb, yb = next(it_b)\n",
    "    except StopIteration:\n",
    "        it_b = iter(train_b_loader); xb, yb = next(it_b)\n",
    "    try: xs, ys = next(it_s)\n",
    "    except StopIteration:\n",
    "        it_s = iter(train_s_loader); xs, ys = next(it_s)\n",
    "\n",
    "    xb, yb, xs, ys = xb.to(device), yb.to(device), xs.to(device), ys.to(device)\n",
    "\n",
    "    opt_b.zero_grad(set_to_none=True)\n",
    "    opt_s.zero_grad(set_to_none=True)\n",
    "\n",
    "    with ac:\n",
    "        yhb = finite_or_zero(baseline(xb))\n",
    "        mse_b = F.mse_loss(yhb, yb)\n",
    "        ang_b = angle_loss(yhb, yb, eps=eps, mask=speed_mask(yb))\n",
    "        loss_b = w_mse * mse_b + w_ang * ang_b\n",
    "\n",
    "        yhs = finite_or_zero(spherical(xs))\n",
    "        ys_r = ys if yhs.shape[-2:] == ys.shape[-2:] else F.interpolate(ys, size=yhs.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        mse_s = F.mse_loss(yhs, ys_r)\n",
    "        ang_s = angle_loss(yhs, ys_r, eps=eps, mask=speed_mask(ys_r))\n",
    "        loss_s = w_mse * mse_s + w_ang * ang_s\n",
    "\n",
    "    if scaler_b.is_enabled():\n",
    "        scaler_b.scale(loss_b).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(baseline.parameters(), 1.0)\n",
    "        scaler_b.step(opt_b); scaler_b.update()\n",
    "    else:\n",
    "        loss_b.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(baseline.parameters(), 1.0)\n",
    "        opt_b.step()\n",
    "\n",
    "    if scaler_s.is_enabled():\n",
    "        scaler_s.scale(loss_s).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(spherical.parameters(), 1.0)\n",
    "        scaler_s.step(opt_s); scaler_s.update()\n",
    "    else:\n",
    "        loss_s.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(spherical.parameters(), 1.0)\n",
    "        opt_s.step()\n",
    "\n",
    "    if (step % print_every) == 0 or step == steps_cap - 1:\n",
    "        dt = time.time() - t0\n",
    "        print(f\"Step {step+1}/{steps_cap} | dt={dt:.1f}s | baseline {loss_b.detach().float().cpu().item():.4f} | spherical {loss_s.detach().float().cpu().item():.4f}\")\n",
    "\n",
    "baseline.eval(); spherical.eval()\n",
    "print(\"Training pass complete (angle+MSE, AMP-safe).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ed41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7s: Robust spherical model scaffold ensuring gradient flow (Windows/AMP-safe)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SphericalModel(nn.Module):\n",
    "    def __init__(self, in_ch=4, hidden_ch=32, out_ch=2, H=64, W=128, use_e3nn=True):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.hidden_ch = hidden_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.use_e3nn = use_e3nn\n",
    "\n",
    "        # Head: parameterized\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, hidden_ch, kernel_size=3, padding=1, bias=True),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Spherical/e3nn block placeholder:\n",
    "        # Replace the below with your actual e3nn pipeline if available.\n",
    "        # Important: keep it as nn.Modules; no no_grad/detach; run in float32 for stability under AMP.\n",
    "        if self.use_e3nn:\n",
    "            self.sph_block = nn.Sequential(\n",
    "                nn.Conv2d(hidden_ch, hidden_ch, 3, padding=1, bias=True),\n",
    "                nn.GELU(),\n",
    "                nn.Conv2d(hidden_ch, hidden_ch, 3, padding=1, bias=True),\n",
    "                nn.GELU(),\n",
    "            )\n",
    "        else:\n",
    "            self.sph_block = nn.Sequential(\n",
    "                nn.Conv2d(hidden_ch, hidden_ch, 3, padding=1, bias=True),\n",
    "                nn.GELU(),\n",
    "                nn.Conv2d(hidden_ch, hidden_ch, 3, padding=1, bias=True),\n",
    "                nn.GELU(),\n",
    "            )\n",
    "\n",
    "        # Tail: parameterized\n",
    "        self.tail = nn.Conv2d(hidden_ch, out_ch, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,C,H,W]\n",
    "        # Never use no_grad() or detach() here.\n",
    "        # Enforce internal grid to a known size\n",
    "        if x.shape[-2:] != (self.H, self.W):\n",
    "            x = F.interpolate(x, size=(self.H, self.W), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        h = self.head(x)  # parameterized\n",
    "\n",
    "        # If using e3nn, perform core ops in float32 for stability, then cast back\n",
    "        if self.use_e3nn:\n",
    "            h32 = h.float()\n",
    "            h32 = self.sph_block(h32)  # Replace with true e3nn graph ops as needed\n",
    "            h = h32.to(h.dtype)\n",
    "        else:\n",
    "            h = self.sph_block(h)\n",
    "\n",
    "        y = self.tail(h)  # parameterized\n",
    "\n",
    "        # Sanity: ensure graph connectivity\n",
    "        assert any(p.requires_grad for p in self.parameters()), \"All params frozen in spherical model.\"\n",
    "        assert y.requires_grad, \"Output is not connected to parameters; check for inadvertent detach/no_grad.\"\n",
    "\n",
    "        return y\n",
    "\n",
    "# Note:\n",
    "# - If you already have a SphericalLatLon or SphericalE3NN class, either:\n",
    "#   a) Replace it entirely with SphericalModel above, OR\n",
    "#   b) Copy the forward() pattern into your class and ensure your e3nn modules are used and parameterized.\n",
    "# - After defining this, re-instantiate your 'spherical' model in Cell 10:\n",
    "#   spherical = SphericalModel(in_ch=IN_CH, hidden_ch=HID, out_ch=2, H=64, W=128, use_e3nn=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f371550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 7.5 updated: VARS/T_CTX set, verts ensured, SphericalSpatioTemporal adapter ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.5 (updated): constants + verts + robust SphericalSpatioTemporal adapter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) Constants\n",
    "if \"VARS\" not in globals():\n",
    "    VARS = [\"u\", \"v\"]\n",
    "if \"T_CTX\" not in globals():\n",
    "    T_CTX = 2  # matches your log\n",
    "\n",
    "# 2) Verts placeholder (diagnostic expects it)\n",
    "if \"verts\" not in globals() or not hasattr(verts, \"shape\") or (hasattr(verts, \"shape\") and (len(tuple(verts.shape)) != 2 or verts.shape[1] != 3)):\n",
    "    N = 1024\n",
    "    verts = torch.randn(N, 3)\n",
    "    verts = verts / (verts.norm(dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "# 3) Base spherical backbone reference\n",
    "if \"SphericalModel\" in globals():\n",
    "    BaseSphericalClass = SphericalModel\n",
    "else:\n",
    "    class BaseSphericalClass(nn.Module):\n",
    "        def __init__(self, in_ch=32, hidden_ch=32, out_ch=32, H=64, W=128, **kwargs):\n",
    "            super().__init__()\n",
    "            self.H, self.W = H, W\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, hidden_ch, 3, padding=1, bias=True),\n",
    "                nn.GELU(),\n",
    "                nn.Conv2d(hidden_ch, out_ch, 3, padding=1, bias=True),\n",
    "                nn.GELU(),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            if x.shape[-2:] != (self.H, self.W):\n",
    "                x = F.interpolate(x, size=(self.H, self.W), mode=\"bilinear\", align_corners=False)\n",
    "            return self.net(x)\n",
    "\n",
    "class SphericalSpatioTemporal(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapter to accept [B,T,C,H,W], fuse T and C, run a spherical backbone, and project to C.\n",
    "    Adds a residual around the backbone to ensure gradient connectivity even if backbone is identity-like.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_vars=2, T_ctx=2, hidden_ch=48, H=64, W=128, verts=None, edge_ch=16, **kwargs):\n",
    "        super().__init__()\n",
    "        self.C = n_vars\n",
    "        self.T = T_ctx\n",
    "        self.H, self.W = H, W\n",
    "\n",
    "        # Fuse T*C -> hidden\n",
    "        self.fuse = nn.Conv2d(self.C * self.T, hidden_ch, kernel_size=1, bias=True)\n",
    "\n",
    "        # Backbone operates on hidden_ch -> hidden_ch\n",
    "        self.backbone = BaseSphericalClass(\n",
    "            in_ch=hidden_ch, hidden_ch=hidden_ch, out_ch=hidden_ch, H=H, W=W\n",
    "        )\n",
    "\n",
    "        # Post and head\n",
    "        self.post = nn.GELU()\n",
    "        self.head = nn.Conv2d(hidden_ch, self.C, kernel_size=1, bias=True)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "        # Optional residual projection to guarantee param path\n",
    "        self.res_proj = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):  # x: [B,T,C,H,W]\n",
    "        assert x.ndim == 5, f\"Expected [B,T,C,H,W], got {tuple(x.shape)}\"\n",
    "        B,T,C,H,W = x.shape\n",
    "        assert T == self.T and C == self.C, f\"Expected T={self.T}, C={self.C}; got {T},{C}\"\n",
    "\n",
    "        # Resize to internal grid\n",
    "        if (H,W) != (self.H,self.W):\n",
    "            x = F.interpolate(x.reshape(B*T, C, H, W), size=(self.H,self.W), mode=\"bilinear\", align_corners=False).view(B,T,C,self.H,self.W)\n",
    "            H, W = self.H, self.W\n",
    "\n",
    "        # Fuse time and channels\n",
    "        x2d = x.reshape(B, T*C, H, W)\n",
    "        h = self.fuse(x2d)           # param path\n",
    "\n",
    "        # Backbone in float32 for stability, then cast back\n",
    "        h32 = h.float()\n",
    "        b32 = self.backbone(h32)     # param path\n",
    "        # Residual to ensure connectivity even if backbone outputs zeros\n",
    "        h32 = h32 + self.res_proj(h32)\n",
    "        h = (b32 + h32).to(h.dtype)\n",
    "\n",
    "        y = self.head(self.post(h))  # param path\n",
    "        y = torch.clamp(y, -50, 50)\n",
    "        # No assert here; Cell 10 will check finiteness\n",
    "        return y\n",
    "\n",
    "print(\"Cell 7.5 updated: VARS/T_CTX set, verts ensured, SphericalSpatioTemporal adapter ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600ec05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cell 10: Diagnostics and Model Instantiation ===\n",
      "OK: Inputs present. VARS=['u', 'v'] (C=2), T_CTX=2, verts.shape=(1024, 3)\n",
      "Device: cuda | AMP dtype: torch.bfloat16\n",
      "Baseline grid forced to: 64x128\n",
      "BaselineLatLon init via kwargs: {'n_vars': 2, 'hidden_ch': 16, 'T_ctx': 2, 'H': 64, 'W': 128}\n",
      "SphericalSpatioTemporal instantiated.\n",
      "Params (M): baseline 0.003 | spherical 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hars\\AppData\\Local\\Temp\\ipykernel_24352\\3589954503.py:144: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_b = torch.cuda.amp.GradScaler(enabled=use_scaler)\n",
      "C:\\Users\\hars\\AppData\\Local\\Temp\\ipykernel_24352\\3589954503.py:145: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_s = torch.cuda.amp.GradScaler(enabled=use_scaler)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Forward smoke test failed. This may be due to input shape expectations.\n",
      "Detail: Output is not connected to parameters; check for inadvertent detach/no_grad.\n",
      "=== Cell 10 complete. Models and optimizers are ready. ===\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 (diagnostic, tolerant): robust checks + instantiation for Baseline and Spherical\n",
    "\n",
    "import torch, inspect, math\n",
    "\n",
    "def die(msg):\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "print(\"=== Cell 10: Diagnostics and Model Instantiation ===\")\n",
    "\n",
    "# 1) Required globals (classes, constants)\n",
    "required = [\"BaselineLatLon\", \"SphericalSpatioTemporal\", \"T_CTX\", \"VARS\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    die(\n",
    "        \"Missing definitions before Cell 10: \"\n",
    "        + \", \".join(missing)\n",
    "        + \".\\nRun cells that define model classes (Cell 7/7s + 7.5) and constants (VARS, T_CTX).\"\n",
    "    )\n",
    "\n",
    "# 2) Sanity on constants\n",
    "if not isinstance(VARS, (list, tuple)) or len(VARS) < 2 or not all(isinstance(v, str) for v in VARS):\n",
    "    die(f\"VARS must be a list/tuple of strings with at least 2 items (winds). Got: {VARS}\")\n",
    "if not isinstance(T_CTX, int) or T_CTX <= 0:\n",
    "    die(f\"T_CTX must be a positive int. Got: {T_CTX}\")\n",
    "\n",
    "# 3) verts: auto-create placeholder if missing/invalid (adapter may ignore it, but Cell 10 expects it)\n",
    "try:\n",
    "    import numpy as np\n",
    "except Exception:\n",
    "    np = None\n",
    "\n",
    "def make_placeholder_verts(n=1024):\n",
    "    v = torch.randn(n, 3)\n",
    "    v = v / (v.norm(dim=1, keepdim=True) + 1e-9)\n",
    "    return v\n",
    "\n",
    "if \"verts\" not in globals():\n",
    "    verts = make_placeholder_verts()\n",
    "    print(\"Info: verts not found â€” created placeholder verts:\", tuple(verts.shape))\n",
    "elif not hasattr(verts, \"shape\"):\n",
    "    print(\"Info: verts had no shape â€” replaced with placeholder.\")\n",
    "    verts = make_placeholder_verts()\n",
    "else:\n",
    "    vshape = tuple(verts.shape)\n",
    "    if len(vshape) != 2 or vshape[1] != 3:\n",
    "        print(f\"Info: verts shape {vshape} invalid â€” replaced with placeholder.\")\n",
    "        verts = make_placeholder_verts()\n",
    "    elif not torch.is_tensor(verts):\n",
    "        if np is not None and isinstance(verts, np.ndarray):\n",
    "            verts = torch.from_numpy(verts)\n",
    "        else:\n",
    "            print(f\"Info: verts type {type(verts)} unsupported â€” replaced with placeholder.\")\n",
    "            verts = make_placeholder_verts()\n",
    "\n",
    "verts = verts.contiguous()\n",
    "if verts.dtype not in (torch.float32, torch.float64, torch.bfloat16, torch.float16):\n",
    "    verts = verts.float()\n",
    "\n",
    "print(f\"OK: Inputs present. VARS={VARS} (C={len(VARS)}), T_CTX={T_CTX}, verts.shape={tuple(verts.shape)}\")\n",
    "\n",
    "# 4) Device and AMP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (device == \"cuda\") and torch.cuda.is_bf16_supported()\n",
    "amp_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(f\"Device: {device} | AMP dtype: {amp_dtype}\")\n",
    "\n",
    "# 5) Channels and baseline grid\n",
    "C = len(VARS)  # winds-only = 2\n",
    "H_target, W_target = 64, 128\n",
    "print(f\"Baseline grid forced to: {H_target}x{W_target}\")\n",
    "\n",
    "# 6) Instantiate BaselineLatLon with flexible signature mapping\n",
    "def make_baseline():\n",
    "    sig = inspect.signature(BaselineLatLon.__init__)\n",
    "    params = list(sig.parameters.keys())[1:]  # skip self\n",
    "    mapped = {}\n",
    "    for p in params:\n",
    "        lp = p.lower()\n",
    "        if lp in (\"c_in\",\"in_channels\",\"channels_in\",\"input_channels\",\"cin\",\"n_vars\",\"num_vars\",\"nvars\"):\n",
    "            mapped[p] = C\n",
    "        elif lp in (\"c_out\",\"out_channels\",\"channels_out\",\"output_channels\",\"cout\"):\n",
    "            mapped[p] = C\n",
    "        elif lp in (\"c_hidden\",\"hidden_channels\",\"mid_channels\",\"width\",\"hidden_dim\",\"hidden_size\",\"hidden_ch\"):\n",
    "            mapped[p] = 16\n",
    "        elif lp in (\"t_ctx\",\"tcontext\",\"context\",\"context_steps\",\"t\"):\n",
    "            mapped[p] = T_CTX\n",
    "        elif lp in (\"h\",\"height\"):\n",
    "            mapped[p] = H_target\n",
    "        elif lp in (\"w\",\"width\"):\n",
    "            mapped[p] = W_target\n",
    "        elif lp in (\"grid_size\",\"shape\",\"hw\"):\n",
    "            mapped[p] = (H_target, W_target)\n",
    "        # else: ignore unknowns; they should have defaults\n",
    "    try:\n",
    "        model = BaselineLatLon(**mapped)\n",
    "        print(\"BaselineLatLon init via kwargs:\", mapped)\n",
    "        return model.to(device)\n",
    "    except TypeError as e_kw:\n",
    "        print(\"BaselineLatLon kwargs init failed, trying positional patterns...\")\n",
    "        patterns = [\n",
    "            (\"C_in\",\"C_out\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\",\"T_ctx\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\",\"T_ctx\",\"H\"),\n",
    "            (\"C_in\",\"hidden\",\"C_out\",\"T_ctx\",\"H\",\"W\"),\n",
    "        ]\n",
    "        vals = {\"C_in\":C, \"C_out\":C, \"hidden\":16, \"T_ctx\":T_CTX, \"H\":H_target, \"W\":W_target}\n",
    "        for pat in patterns:\n",
    "            args = [vals[k] for k in pat]\n",
    "            try:\n",
    "                model = BaselineLatLon(*args).to(device)\n",
    "                print(\"BaselineLatLon init via args pattern:\", pat, \"->\", args)\n",
    "                return model\n",
    "            except TypeError:\n",
    "                continue\n",
    "        print(\"BaselineLatLon signature:\", sig)\n",
    "        raise RuntimeError(f\"Could not instantiate BaselineLatLon. Last error: {e_kw}\")\n",
    "\n",
    "baseline = make_baseline()\n",
    "\n",
    "# 7) Instantiate Spherical model (adapter from Cell 7.5)\n",
    "try:\n",
    "    spherical = SphericalSpatioTemporal(\n",
    "        n_vars=C,\n",
    "        verts=verts.to(device) if hasattr(verts, \"to\") else verts,\n",
    "        hidden_ch=48,\n",
    "        edge_ch=16,\n",
    "        T_ctx=T_CTX,\n",
    "        H=H_target, W=W_target\n",
    "    ).to(device)\n",
    "    print(\"SphericalSpatioTemporal instantiated.\")\n",
    "except TypeError as te:\n",
    "    raise RuntimeError(f\"Failed to instantiate SphericalSpatioTemporal: {te}\")\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "# 8) Optimizers (fresh)\n",
    "opt_b = torch.optim.AdamW(baseline.parameters(), lr=1e-3, weight_decay=1e-4, betas=(0.9, 0.999))\n",
    "opt_s = torch.optim.AdamW(spherical.parameters(), lr=1e-3, weight_decay=1e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# 9) AMP scaler (disabled by default for stability; enable if you want fp16)\n",
    "use_scaler = (device == \"cuda\") and (amp_dtype == torch.float16) and False\n",
    "try:\n",
    "    scaler_b = torch.cuda.amp.GradScaler(enabled=use_scaler)\n",
    "    scaler_s = torch.cuda.amp.GradScaler(enabled=use_scaler)\n",
    "except Exception:\n",
    "    scaler_b = torch.amp.GradScaler(enabled=use_scaler)\n",
    "    scaler_s = torch.amp.GradScaler(enabled=use_scaler)\n",
    "\n",
    "# 10) Param counts\n",
    "p_b = sum(p.numel() for p in baseline.parameters())\n",
    "p_s = sum(p.numel() for p in spherical.parameters())\n",
    "print(f\"Params (M): baseline {p_b/1e6:.3f} | spherical {p_s/1e6:.3f}\")\n",
    "\n",
    "# 11) Quick forward smoke tests to catch shape/NaN issues early\n",
    "try:\n",
    "    baseline.eval(); spherical.eval()\n",
    "    with torch.no_grad():\n",
    "        B_test = 1\n",
    "        H_test, W_test = H_target, W_target\n",
    "        # Dummy batch with expected shape [B,T,C,H,W]\n",
    "        x_dummy = torch.zeros(B_test, T_CTX, C, H_test, W_test, device=device)\n",
    "        yb = baseline(x_dummy)        # [B, C, H, W]\n",
    "        ys = spherical(x_dummy)       # [B, C, H, W]\n",
    "        if yb.ndim != 4 or ys.ndim != 4:\n",
    "            die(f\"Unexpected output shapes. Baseline: {tuple(yb.shape)}, Spherical: {tuple(ys.shape)}\")\n",
    "        if not torch.isfinite(yb).all():\n",
    "            die(\"Baseline forward produced non-finite values on zero input.\")\n",
    "        if not torch.isfinite(ys).all():\n",
    "            die(\"Spherical forward produced non-finite values on zero input.\")\n",
    "        print(\"Forward smoke test passed: outputs finite.\")\n",
    "except Exception as e:\n",
    "    print(\"Warning: Forward smoke test failed. This may be due to input shape expectations.\")\n",
    "    print(\"Detail:\", e)\n",
    "\n",
    "print(\"=== Cell 10 complete. Models and optimizers are ready. ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d53763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizers (and schedulers) reset.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Re-instantiate optimizers (keep models as already instantiated)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "baseline = baseline.to(device).train()\n",
    "spherical = spherical.to(device).train()\n",
    "\n",
    "# Fresh optimizers\n",
    "opt_b = torch.optim.AdamW(baseline.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "opt_s = torch.optim.AdamW(spherical.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "\n",
    "# Optional schedulers (plateau safety, same for both; not strictly necessary for 100 steps)\n",
    "sch_b = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_b, mode=\"min\", factor=0.5, patience=20)\n",
    "sch_s = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_s, mode=\"min\", factor=0.5, patience=20)\n",
    "\n",
    "print(\"Optimizers (and schedulers) reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2454521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/100 | dt=16.6s | baseline 0.9930 (EMA 0.9930) | spherical 1.1841 (EMA 1.1841) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 2/100 | dt=27.5s | baseline 1.2877 (EMA 1.0225) | spherical 1.1238 (EMA 1.1781) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 3/100 | dt=38.3s | baseline 1.3499 (EMA 1.0552) | spherical 1.0671 (EMA 1.1670) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 4/100 | dt=49.0s | baseline 1.2504 (EMA 1.0748) | spherical 0.9725 (EMA 1.1475) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 5/100 | dt=59.9s | baseline 0.9065 (EMA 1.0579) | spherical 0.9172 (EMA 1.1245) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 6/100 | dt=70.7s | baseline 1.1578 (EMA 1.0679) | spherical 0.8758 (EMA 1.0996) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 7/100 | dt=81.6s | baseline 1.0990 (EMA 1.0710) | spherical 0.7433 (EMA 1.0640) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 8/100 | dt=92.5s | baseline 1.0643 (EMA 1.0704) | spherical 0.6151 (EMA 1.0191) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 9/100 | dt=103.3s | baseline 1.1553 (EMA 1.0789) | spherical 1.0364 (EMA 1.0208) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 10/100 | dt=114.2s | baseline 1.0118 (EMA 1.0721) | spherical 0.6244 (EMA 0.9812) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 11/100 | dt=125.0s | baseline 1.0072 (EMA 1.0657) | spherical 0.6429 (EMA 0.9473) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 12/100 | dt=135.9s | baseline 0.8560 (EMA 1.0447) | spherical 0.9449 (EMA 0.9471) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 13/100 | dt=146.8s | baseline 0.9740 (EMA 1.0376) | spherical 0.6831 (EMA 0.9207) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 14/100 | dt=157.6s | baseline 0.9086 (EMA 1.0247) | spherical 0.9240 (EMA 0.9210) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 15/100 | dt=168.6s | baseline 1.1704 (EMA 1.0393) | spherical 0.7354 (EMA 0.9025) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 16/100 | dt=179.6s | baseline 0.9894 (EMA 1.0343) | spherical 0.5127 (EMA 0.8635) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 17/100 | dt=190.6s | baseline 1.0364 (EMA 1.0345) | spherical 0.7429 (EMA 0.8514) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 18/100 | dt=201.8s | baseline 0.9616 (EMA 1.0272) | spherical 0.9070 (EMA 0.8570) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 19/100 | dt=213.0s | baseline 0.7814 (EMA 1.0026) | spherical 0.6580 (EMA 0.8371) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 20/100 | dt=224.3s | baseline 0.9587 (EMA 0.9983) | spherical 0.9962 (EMA 0.8530) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 21/100 | dt=235.3s | baseline 0.9351 (EMA 0.9919) | spherical 0.9635 (EMA 0.8640) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 22/100 | dt=246.2s | baseline 0.8849 (EMA 0.9812) | spherical 0.5810 (EMA 0.8357) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 23/100 | dt=257.2s | baseline 0.7366 (EMA 0.9568) | spherical 0.5643 (EMA 0.8086) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 24/100 | dt=268.2s | baseline 0.7113 (EMA 0.9322) | spherical 0.8483 (EMA 0.8126) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 25/100 | dt=279.4s | baseline 0.7591 (EMA 0.9149) | spherical 0.9177 (EMA 0.8231) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 26/100 | dt=290.5s | baseline 0.6760 (EMA 0.8910) | spherical 0.5707 (EMA 0.7978) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 27/100 | dt=301.7s | baseline 0.7082 (EMA 0.8727) | spherical 0.7216 (EMA 0.7902) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 28/100 | dt=313.0s | baseline 0.9303 (EMA 0.8785) | spherical 0.8264 (EMA 0.7938) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 29/100 | dt=324.4s | baseline 0.9708 (EMA 0.8877) | spherical 0.5049 (EMA 0.7649) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 30/100 | dt=335.9s | baseline 0.9197 (EMA 0.8909) | spherical 0.7799 (EMA 0.7664) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 31/100 | dt=347.1s | baseline 0.5914 (EMA 0.8610) | spherical 0.5650 (EMA 0.7463) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 32/100 | dt=358.0s | baseline 0.6917 (EMA 0.8441) | spherical 0.4729 (EMA 0.7189) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 33/100 | dt=390.4s | baseline 0.8636 (EMA 0.8460) | spherical 0.9658 (EMA 0.7436) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 34/100 | dt=401.4s | baseline 1.0569 (EMA 0.8671) | spherical 0.7890 (EMA 0.7482) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 35/100 | dt=412.3s | baseline 0.7233 (EMA 0.8527) | spherical 0.4774 (EMA 0.7211) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 36/100 | dt=424.0s | baseline 0.9499 (EMA 0.8624) | spherical 0.9946 (EMA 0.7484) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 37/100 | dt=435.6s | baseline 1.0381 (EMA 0.8800) | spherical 0.4620 (EMA 0.7198) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 38/100 | dt=447.1s | baseline 1.0216 (EMA 0.8942) | spherical 0.5395 (EMA 0.7018) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 39/100 | dt=458.6s | baseline 0.7629 (EMA 0.8810) | spherical 1.1452 (EMA 0.7461) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 40/100 | dt=470.0s | baseline 1.0092 (EMA 0.8938) | spherical 0.4677 (EMA 0.7183) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 41/100 | dt=481.6s | baseline 0.6383 (EMA 0.8683) | spherical 0.4659 (EMA 0.6930) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 42/100 | dt=493.4s | baseline 0.9591 (EMA 0.8774) | spherical 0.8649 (EMA 0.7102) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 43/100 | dt=505.2s | baseline 0.9473 (EMA 0.8844) | spherical 0.4191 (EMA 0.6811) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 44/100 | dt=516.7s | baseline 0.9221 (EMA 0.8881) | spherical 0.3471 (EMA 0.6477) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 45/100 | dt=528.2s | baseline 1.0122 (EMA 0.9006) | spherical 1.0738 (EMA 0.6903) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 46/100 | dt=539.5s | baseline 0.8262 (EMA 0.8931) | spherical 0.3967 (EMA 0.6610) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 47/100 | dt=550.4s | baseline 0.8503 (EMA 0.8888) | spherical 1.0349 (EMA 0.6983) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 48/100 | dt=561.2s | baseline 0.8287 (EMA 0.8828) | spherical 0.3539 (EMA 0.6639) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 49/100 | dt=572.1s | baseline 0.7437 (EMA 0.8689) | spherical 1.1436 (EMA 0.7119) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 50/100 | dt=583.0s | baseline 0.7844 (EMA 0.8605) | spherical 0.9644 (EMA 0.7371) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 51/100 | dt=593.9s | baseline 0.7345 (EMA 0.8479) | spherical 0.3087 (EMA 0.6943) | LR_b=1.00e-03 LR_s=1.00e-03\n",
      "Step 52/100 | dt=604.8s | baseline 0.7576 (EMA 0.8388) | spherical 0.3655 (EMA 0.6614) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 53/100 | dt=615.6s | baseline 0.6284 (EMA 0.8178) | spherical 0.4271 (EMA 0.6380) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 54/100 | dt=627.0s | baseline 0.6165 (EMA 0.7977) | spherical 0.5857 (EMA 0.6327) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 55/100 | dt=637.2s | baseline 0.9802 (EMA 0.8159) | spherical 1.0763 (EMA 0.6771) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 56/100 | dt=647.4s | baseline 0.7767 (EMA 0.8120) | spherical 1.0518 (EMA 0.7146) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 57/100 | dt=657.7s | baseline 0.8113 (EMA 0.8119) | spherical 0.4477 (EMA 0.6879) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 58/100 | dt=667.8s | baseline 0.7793 (EMA 0.8087) | spherical 0.4006 (EMA 0.6592) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 59/100 | dt=678.1s | baseline 0.5789 (EMA 0.7857) | spherical 0.6101 (EMA 0.6543) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 60/100 | dt=688.3s | baseline 0.7444 (EMA 0.7816) | spherical 0.5952 (EMA 0.6483) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 61/100 | dt=698.5s | baseline 0.7694 (EMA 0.7803) | spherical 0.4658 (EMA 0.6301) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 62/100 | dt=708.7s | baseline 0.7632 (EMA 0.7786) | spherical 0.8932 (EMA 0.6564) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 63/100 | dt=718.9s | baseline 0.5787 (EMA 0.7586) | spherical 0.6705 (EMA 0.6578) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 64/100 | dt=729.2s | baseline 0.7467 (EMA 0.7574) | spherical 0.4466 (EMA 0.6367) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 65/100 | dt=759.7s | baseline 0.8111 (EMA 0.7628) | spherical 0.5255 (EMA 0.6256) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 66/100 | dt=770.1s | baseline 0.8538 (EMA 0.7719) | spherical 0.5716 (EMA 0.6202) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 67/100 | dt=780.5s | baseline 0.7861 (EMA 0.7733) | spherical 0.8392 (EMA 0.6421) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 68/100 | dt=790.7s | baseline 0.7951 (EMA 0.7755) | spherical 0.8858 (EMA 0.6665) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 69/100 | dt=800.9s | baseline 0.6847 (EMA 0.7664) | spherical 0.8987 (EMA 0.6897) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 70/100 | dt=811.2s | baseline 0.8850 (EMA 0.7783) | spherical 0.6827 (EMA 0.6890) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 71/100 | dt=821.4s | baseline 0.8846 (EMA 0.7889) | spherical 0.6356 (EMA 0.6836) | LR_b=5.00e-04 LR_s=1.00e-03\n",
      "Step 72/100 | dt=831.6s | baseline 0.7731 (EMA 0.7873) | spherical 0.5618 (EMA 0.6715) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 73/100 | dt=841.9s | baseline 0.7624 (EMA 0.7848) | spherical 0.7389 (EMA 0.6782) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 74/100 | dt=852.1s | baseline 0.8513 (EMA 0.7915) | spherical 0.4989 (EMA 0.6603) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 75/100 | dt=862.3s | baseline 0.6559 (EMA 0.7779) | spherical 0.6192 (EMA 0.6562) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 76/100 | dt=872.5s | baseline 0.9094 (EMA 0.7911) | spherical 0.5095 (EMA 0.6415) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 77/100 | dt=882.8s | baseline 0.7721 (EMA 0.7892) | spherical 0.7261 (EMA 0.6500) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 78/100 | dt=893.0s | baseline 0.8566 (EMA 0.7959) | spherical 0.6280 (EMA 0.6478) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 79/100 | dt=903.3s | baseline 0.6919 (EMA 0.7855) | spherical 1.0239 (EMA 0.6854) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 80/100 | dt=913.5s | baseline 0.8482 (EMA 0.7918) | spherical 0.5818 (EMA 0.6750) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 81/100 | dt=923.7s | baseline 0.8441 (EMA 0.7970) | spherical 0.5832 (EMA 0.6658) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 82/100 | dt=934.0s | baseline 0.8616 (EMA 0.8035) | spherical 0.8945 (EMA 0.6887) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 83/100 | dt=944.2s | baseline 0.6687 (EMA 0.7900) | spherical 0.6824 (EMA 0.6881) | LR_b=5.00e-04 LR_s=5.00e-04\n",
      "Step 84/100 | dt=954.4s | baseline 0.7276 (EMA 0.7838) | spherical 0.5256 (EMA 0.6718) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 85/100 | dt=964.7s | baseline 0.7967 (EMA 0.7850) | spherical 0.5102 (EMA 0.6557) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 86/100 | dt=974.9s | baseline 0.9666 (EMA 0.8032) | spherical 0.7478 (EMA 0.6649) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 87/100 | dt=985.1s | baseline 0.6688 (EMA 0.7898) | spherical 0.7793 (EMA 0.6763) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 88/100 | dt=995.3s | baseline 0.7645 (EMA 0.7872) | spherical 0.6479 (EMA 0.6735) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 89/100 | dt=1005.5s | baseline 0.6241 (EMA 0.7709) | spherical 0.6376 (EMA 0.6699) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 90/100 | dt=1015.9s | baseline 0.6014 (EMA 0.7540) | spherical 0.7256 (EMA 0.6755) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 91/100 | dt=1026.3s | baseline 0.5958 (EMA 0.7381) | spherical 0.4472 (EMA 0.6526) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 92/100 | dt=1036.7s | baseline 0.6171 (EMA 0.7260) | spherical 0.6487 (EMA 0.6522) | LR_b=2.50e-04 LR_s=5.00e-04\n",
      "Step 93/100 | dt=1047.1s | baseline 0.7069 (EMA 0.7241) | spherical 0.5418 (EMA 0.6412) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Step 94/100 | dt=1057.4s | baseline 0.7929 (EMA 0.7310) | spherical 0.8836 (EMA 0.6654) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Step 95/100 | dt=1067.8s | baseline 0.5756 (EMA 0.7155) | spherical 0.3818 (EMA 0.6371) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Step 96/100 | dt=1078.2s | baseline 0.7765 (EMA 0.7216) | spherical 0.4866 (EMA 0.6220) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Step 97/100 | dt=1109.4s | baseline 0.6220 (EMA 0.7116) | spherical 0.6179 (EMA 0.6216) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Step 98/100 | dt=1119.8s | baseline 0.6222 (EMA 0.7027) | spherical 0.5350 (EMA 0.6129) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Step 99/100 | dt=1130.2s | baseline 0.6383 (EMA 0.6962) | spherical 0.3723 (EMA 0.5889) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Step 100/100 | dt=1140.5s | baseline 0.8334 (EMA 0.7100) | spherical 0.7855 (EMA 0.6085) | LR_b=2.50e-04 LR_s=2.50e-04\n",
      "Training complete (100 steps; minimal fair + rotation augmentation).\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Minimal, fair training for 100 steps â€” rotation augmentation + stable objective\n",
    "# Identical LR, capacity, AMP; single forward per model per batch; shared normalization.\n",
    "\n",
    "import time, math, random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from contextlib import nullcontext\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (device == \"cuda\") and torch.cuda.is_bf16_supported()\n",
    "amp_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "baseline.train(); spherical.train()\n",
    "for p in baseline.parameters(): p.requires_grad_(True)\n",
    "for p in spherical.parameters(): p.requires_grad_(True)\n",
    "\n",
    "# Hyperparameters (identical)\n",
    "total_steps = 100\n",
    "print_every = 10\n",
    "time_print_guard = 10.0\n",
    "base_lr = 1.0e-3\n",
    "for g in opt_b.param_groups: g[\"lr\"] = base_lr\n",
    "for g in opt_s.param_groups: g[\"lr\"] = base_lr\n",
    "max_grad_norm = 1.0\n",
    "grad_noise_std = 5e-5  # tiny noise to stabilize\n",
    "\n",
    "# AMP (new API)\n",
    "if device == \"cuda\":\n",
    "    ac = torch.amp.autocast(device_type=\"cuda\", dtype=amp_dtype)\n",
    "    scaler_b = torch.amp.GradScaler(\"cuda\", enabled=(amp_dtype == torch.float16))\n",
    "    scaler_s = torch.amp.GradScaler(\"cuda\", enabled=(amp_dtype == torch.float16))\n",
    "else:\n",
    "    ac = nullcontext()\n",
    "    scaler_b = torch.amp.GradScaler(enabled=False)\n",
    "    scaler_s = torch.amp.GradScaler(enabled=False)\n",
    "\n",
    "# Loss weights (identical; slight warmup on angle term for first 30 steps)\n",
    "w_mse = 1.0\n",
    "w_ang_main = 0.20\n",
    "w_ang_warm = 0.10  # used for steps <= 30\n",
    "w_mag = 0.10\n",
    "\n",
    "# Common grid\n",
    "Hc, Wc = 64, 128\n",
    "common_size = (Hc, Wc)\n",
    "\n",
    "# Latitude weights (cos Ï†), normalized to mean 1\n",
    "phi = torch.linspace(-math.pi/2, math.pi/2, steps=Hc, device=device).view(1, 1, Hc, 1)\n",
    "cos_phi = torch.cos(phi).clamp(min=1e-3)\n",
    "w_lat = cos_phi / cos_phi.mean()\n",
    "\n",
    "def weight_reduce(x, w):\n",
    "    if x.dim() == 4 and x.size(1) != 1:\n",
    "        x = x.mean(dim=1, keepdim=True)\n",
    "    return (x * w).mean()\n",
    "\n",
    "def smooth_angle_loss(pred_uv, true_uv, eps=1e-6, w=None):\n",
    "    pu, pv = pred_uv[:, 0], pred_uv[:, 1]\n",
    "    tu, tv = true_uv[:, 0], true_uv[:, 1]\n",
    "    pn = torch.sqrt(pu * pu + pv * pv + eps)\n",
    "    tn = torch.sqrt(tu * tu + tv * tv + eps)\n",
    "    cosv = (pu * tu + pv * tv) / (pn * tn + eps)\n",
    "    cosv = torch.clamp(cosv, -1.0, 1.0)\n",
    "    sin_half = torch.sqrt((1 - cosv) * 0.5)\n",
    "    loss_map = sin_half.unsqueeze(1)\n",
    "    return weight_reduce(loss_map, w) if w is not None else loss_map.mean()\n",
    "\n",
    "def normalize(y, mean, var):\n",
    "    return (y - mean) / torch.sqrt(var + 1e-8)\n",
    "\n",
    "def finite_or_zero(t):\n",
    "    return torch.where(torch.isfinite(t), t, torch.zeros_like(t))\n",
    "\n",
    "# Shared running stats\n",
    "running_mean = None\n",
    "running_var = None\n",
    "momentum = 0.1\n",
    "warmup_stats_steps = 20\n",
    "\n",
    "def update_running_stats(y_batch, running_mean, running_var):\n",
    "    mean = y_batch.mean(dim=[0,2,3], keepdim=True)\n",
    "    var = y_batch.var(dim=[0,2,3], unbiased=False, keepdim=True)\n",
    "    if running_mean is None:\n",
    "        return mean.detach(), (var + 1e-8).detach()\n",
    "    rm = (1 - momentum) * running_mean + momentum * mean.detach()\n",
    "    rv = (1 - momentum) * running_var + momentum * (var + 1e-8).detach()\n",
    "    return rm, rv\n",
    "\n",
    "# Rotation augmentation (lat/lon rolls)\n",
    "def random_rot_params(H, W):\n",
    "    k_lon = random.randint(0, max(0, W//18))     # up to ~20Â°\n",
    "    k_lat = random.randint(-max(1, H//36), max(1, H//36))  # up to ~5Â°\n",
    "    return k_lat, k_lon\n",
    "\n",
    "def apply_rot(x, k_lat, k_lon):\n",
    "    if k_lat != 0:\n",
    "        x = torch.roll(x, shifts=k_lat, dims=2)\n",
    "    if k_lon != 0:\n",
    "        x = torch.roll(x, shifts=k_lon, dims=3)\n",
    "    return x\n",
    "\n",
    "# Connectivity smoke test for spherical\n",
    "def sanity_connectivity(model, x_sample):\n",
    "    model.train()\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None: p.grad.zero_()\n",
    "    y = model(x_sample)\n",
    "    loss = (y**2).mean()\n",
    "    loss.backward()\n",
    "    nz = sum(1 for _,p in model.named_parameters()\n",
    "             if (p.grad is not None and torch.isfinite(p.grad).all() and p.grad.abs().sum() > 0))\n",
    "    assert nz > 0, \"Model produced no parameter gradients.\"\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xs_chk, _ = next(iter(train_s_loader))\n",
    "xs_chk = xs_chk.to(device)[:1]\n",
    "sanity_connectivity(spherical, xs_chk)\n",
    "\n",
    "# Data iters\n",
    "it_b = iter(train_b_loader)\n",
    "it_s = iter(train_s_loader)\n",
    "\n",
    "# EMA logging\n",
    "ema_b = None\n",
    "ema_s = None\n",
    "ema_beta = 0.9\n",
    "\n",
    "t0 = time.time()\n",
    "last_print = t0\n",
    "\n",
    "for step in range(1, total_steps + 1):\n",
    "    try:\n",
    "        xb, yb = next(it_b)\n",
    "    except StopIteration:\n",
    "        it_b = iter(train_b_loader); xb, yb = next(it_b)\n",
    "    try:\n",
    "        xs, ys = next(it_s)\n",
    "    except StopIteration:\n",
    "        it_s = iter(train_s_loader); xs, ys = next(it_s)\n",
    "\n",
    "    xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "    xs, ys = xs.to(device, non_blocking=True), ys.to(device, non_blocking=True)\n",
    "\n",
    "    # Rotation augmentation: same transform for input and target (applied per branch)\n",
    "    k_lat_b, k_lon_b = random_rot_params(Hc, Wc)\n",
    "    k_lat_s, k_lon_s = random_rot_params(Hc, Wc)\n",
    "\n",
    "    # Targets on common grid + aug\n",
    "    yb_c = F.interpolate(yb, size=common_size, mode=\"bilinear\", align_corners=False)\n",
    "    ys_c = F.interpolate(ys, size=common_size, mode=\"bilinear\", align_corners=False)\n",
    "    yb_c = apply_rot(yb_c, k_lat_b, k_lon_b)\n",
    "    ys_c = apply_rot(ys_c, k_lat_s, k_lon_s)\n",
    "\n",
    "    opt_b.zero_grad(set_to_none=True)\n",
    "    opt_s.zero_grad(set_to_none=True)\n",
    "\n",
    "    with ac:\n",
    "        # Forward on rotated inputs\n",
    "        yhb = finite_or_zero(baseline(apply_rot(xb, k_lat_b, k_lon_b)))\n",
    "        yhs = finite_or_zero(spherical(apply_rot(xs, k_lat_s, k_lon_s)))\n",
    "\n",
    "        # Align preds to common grid\n",
    "        yhb_c = F.interpolate(yhb, size=common_size, mode=\"bilinear\", align_corners=False)\n",
    "        yhs_c = F.interpolate(yhs, size=common_size, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Update shared normalization\n",
    "        if step <= warmup_stats_steps:\n",
    "            cat_targets = torch.cat([yb_c.detach(), ys_c.detach()], dim=0)\n",
    "            running_mean, running_var = update_running_stats(cat_targets, running_mean, running_var)\n",
    "\n",
    "        if running_mean is not None:\n",
    "            yhb_n = normalize(yhb_c, running_mean, running_var)\n",
    "            yhs_n = normalize(yhs_c, running_mean, running_var)\n",
    "            yb_n  = normalize(yb_c,  running_mean, running_var)\n",
    "            ys_n  = normalize(ys_c,  running_mean, running_var)\n",
    "        else:\n",
    "            yhb_n, yhs_n, yb_n, ys_n = yhb_c, yhs_c, yb_c, ys_c\n",
    "\n",
    "        # Losses (identical)\n",
    "        w_ang = w_ang_warm if step <= 30 else w_ang_main\n",
    "\n",
    "        mse_b = weight_reduce((yhb_n - yb_n).pow(2), w_lat)\n",
    "        mse_s = weight_reduce((yhs_n - ys_n).pow(2), w_lat)\n",
    "\n",
    "        ang_b = smooth_angle_loss(yhb_n, yb_n, w=w_lat)\n",
    "        ang_s = smooth_angle_loss(yhs_n, ys_n, w=w_lat)\n",
    "\n",
    "        spd_b = torch.sqrt((yhb_n[:,0:1]**2 + yhb_n[:,1:2]**2) + 1e-8)\n",
    "        spd_t_b = torch.sqrt((yb_n[:,0:1]**2 + yb_n[:,1:2]**2) + 1e-8)\n",
    "        mag_b = weight_reduce((spd_b - spd_t_b).abs(), w_lat)\n",
    "\n",
    "        spd_s = torch.sqrt((yhs_n[:,0:1]**2 + yhs_n[:,1:2]**2) + 1e-8)\n",
    "        spd_t_s = torch.sqrt((ys_n[:,0:1]**2 + ys_n[:,1:2]**2) + 1e-8)\n",
    "        mag_s = weight_reduce((spd_s - spd_t_s).abs(), w_lat)\n",
    "\n",
    "        loss_b = w_mse*mse_b + w_ang*ang_b + w_mag*mag_b\n",
    "        loss_s = w_mse*mse_s + w_ang*ang_s + w_mag*mag_s\n",
    "\n",
    "    # Backward & step (spherical first)\n",
    "    if scaler_s.is_enabled():\n",
    "        scaler_s.scale(loss_s).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(spherical.parameters(), max_grad_norm)\n",
    "        with torch.no_grad():\n",
    "            for p in spherical.parameters():\n",
    "                if p.grad is not None and torch.isfinite(p.grad).all():\n",
    "                    p.grad.add_(torch.randn_like(p.grad) * grad_noise_std)\n",
    "        scaler_s.step(opt_s); scaler_s.update()\n",
    "    else:\n",
    "        loss_s.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(spherical.parameters(), max_grad_norm)\n",
    "        with torch.no_grad():\n",
    "            for p in spherical.parameters():\n",
    "                if p.grad is not None and torch.isfinite(p.grad).all():\n",
    "                    p.grad.add_(torch.randn_like(p.grad) * grad_noise_std)\n",
    "        opt_s.step()\n",
    "\n",
    "    if scaler_b.is_enabled():\n",
    "        scaler_b.scale(loss_b).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(baseline.parameters(), max_grad_norm)\n",
    "        with torch.no_grad():\n",
    "            for p in baseline.parameters():\n",
    "                if p.grad is not None and torch.isfinite(p.grad).all():\n",
    "                    p.grad.add_(torch.randn_like(p.grad) * grad_noise_std)\n",
    "        scaler_b.step(opt_b); scaler_b.update()\n",
    "    else:\n",
    "        loss_b.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(baseline.parameters(), max_grad_norm)\n",
    "        with torch.no_grad():\n",
    "            for p in baseline.parameters():\n",
    "                if p.grad is not None and torch.isfinite(p.grad).all():\n",
    "                    p.grad.add_(torch.randn_like(p.grad) * grad_noise_std)\n",
    "        opt_b.step()\n",
    "\n",
    "    # Optional schedulers (same trigger)\n",
    "    if 'sch_b' in globals(): sch_b.step(float(loss_b.detach()))\n",
    "    if 'sch_s' in globals(): sch_s.step(float(loss_s.detach()))\n",
    "\n",
    "    # Logging with EMA\n",
    "    lb = float(loss_b.detach().cpu()); ls = float(loss_s.detach().cpu())\n",
    "    ema_b = lb if ema_b is None else (0.9*ema_b + 0.1*lb)\n",
    "    ema_s = ls if ema_s is None else (0.9*ema_s + 0.1*ls)\n",
    "\n",
    "    now = time.time()\n",
    "    if (step % print_every) == 0 or (now - last_print) > time_print_guard or step == 1 or step == total_steps:\n",
    "        dt = now - t0\n",
    "        print(f\"Step {step}/{total_steps} | dt={dt:.1f}s | baseline {lb:.4f} (EMA {ema_b:.4f}) | spherical {ls:.4f} (EMA {ema_s:.4f}) | LR_b={opt_b.param_groups[0]['lr']:.2e} LR_s={opt_s.param_groups[0]['lr']:.2e}\")\n",
    "        last_print = now\n",
    "\n",
    "baseline.eval(); spherical.eval()\n",
    "print(\"Training complete (100 steps; minimal fair + rotation augmentation).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35640501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SphericalModel.forward patched successfully (training-only asserts).\n"
     ]
    }
   ],
   "source": [
    "# Patch: make SphericalModel assertions training-only with proper instance binding\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "assert \"SphericalModel\" in globals(), \"SphericalModel must be defined before this patch.\"\n",
    "\n",
    "def sphericalmodel_forward_training_only_asserts(self, x):\n",
    "    # x: [B,C,H,W]\n",
    "    if x.shape[-2:] != (self.H, self.W):\n",
    "        x = F.interpolate(x, size=(self.H, self.W), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    h = self.head(x)  # parameterized\n",
    "\n",
    "    if self.use_e3nn:\n",
    "        h32 = h.float()\n",
    "        h32 = self.sph_block(h32)\n",
    "        h = h32.to(h.dtype)\n",
    "    else:\n",
    "        h = self.sph_block(h)\n",
    "\n",
    "    y = self.tail(h)  # parameterized\n",
    "\n",
    "    # Only assert connectivity in training mode\n",
    "    if self.training:\n",
    "        # if every param is frozen, that's suspicious\n",
    "        if not any(p.requires_grad for p in self.parameters()):\n",
    "            raise AssertionError(\"All params frozen in spherical model.\")\n",
    "        # In training, output should require grad if graph is connected\n",
    "        if not getattr(y, \"requires_grad\", False):\n",
    "            raise AssertionError(\"Output is not connected to parameters; check for inadvertent detach/no_grad.\")\n",
    "\n",
    "    return y\n",
    "\n",
    "# Properly assign the function as an instance method on the class\n",
    "SphericalModel.forward = sphericalmodel_forward_training_only_asserts\n",
    "\n",
    "print(\"SphericalModel.forward patched successfully (training-only asserts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1d509be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast eval (Cell 12) complete.\n",
      "Baseline score: 0.679794 | Spherical score: 0.667570 | Winner: spherical\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Safe minimal eval with extra metrics (CPU scalars only, no plots)\n",
    "\n",
    "import math, gc, traceback\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def safe_print(msg):\n",
    "    try:\n",
    "        print(msg, flush=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "C = len(VARS) if \"VARS\" in globals() else 2\n",
    "Hc, Wc = 64, 128\n",
    "\n",
    "def weight_reduce(x, w):\n",
    "    # x: [B,C,H,W] or [B,1,H,W] -> scalar\n",
    "    if x.dim() == 4 and x.size(1) != 1:\n",
    "        x = x.mean(dim=1, keepdim=True)\n",
    "    return (x * w).mean()\n",
    "\n",
    "def smooth_angle_loss(pred_uv, true_uv, eps=1e-6, w=None):\n",
    "    pu, pv = pred_uv[:, 0], pred_uv[:, 1]\n",
    "    tu, tv = true_uv[:, 0], true_uv[:, 1]\n",
    "    pn = torch.sqrt(pu * pu + pv * pv + eps)\n",
    "    tn = torch.sqrt(tu * tu + tv * tv + eps)\n",
    "    cosv = (pu * tu + pv * tv) / (pn * tn + eps)\n",
    "    cosv = torch.clamp(cosv, -1.0, 1.0)\n",
    "    sin_half = torch.sqrt((1 - cosv) * 0.5)\n",
    "    loss_map = sin_half.unsqueeze(1)  # [B,1,H,W]\n",
    "    return weight_reduce(loss_map, w) if w is not None else loss_map.mean()\n",
    "\n",
    "def l1(x):  # mean absolute\n",
    "    return x.abs().mean()\n",
    "\n",
    "def rmse(x):  # root mean squared\n",
    "    return torch.sqrt(torch.clamp(x.pow(2).mean(), min=1e-12))\n",
    "\n",
    "def div2d(u, v):\n",
    "    # very light divergence estimate via Sobel-like finite diff; periodic in lon\n",
    "    # inputs: [B,1,H,W], returns [B,1,H,W]\n",
    "    dudx = u[..., :, [1, *range(2, u.size(-1)), 0]] - u  # forward diff with wrap on lon\n",
    "    dvdy = v[..., [1, *range(2, v.size(-2)), 0], :] - v  # forward diff in lat (no wrap but tiny batch)\n",
    "    return dudx + dvdy\n",
    "\n",
    "# Latitude weights (on device used for compute)\n",
    "phi = torch.linspace(-math.pi/2, math.pi/2, steps=Hc, device=device).view(1, 1, Hc, 1)\n",
    "cos_phi = torch.cos(phi).clamp(min=1e-3)\n",
    "w_lat = cos_phi / cos_phi.mean()\n",
    "\n",
    "# Tiny batch from val (fallback to train). Expect loaders that yield [B,T,C,H,W].\n",
    "def get_small_batch():\n",
    "    try:\n",
    "        xb, yb = next(iter(val_b_loader))\n",
    "        xs, ys = next(iter(val_s_loader))\n",
    "        return xb, yb, xs, ys\n",
    "    except Exception:\n",
    "        xb, yb = next(iter(train_b_loader))\n",
    "        xs, ys = next(iter(train_s_loader))\n",
    "        return xb, yb, xs, ys\n",
    "\n",
    "try:\n",
    "    xb, yb, xs, ys = get_small_batch()\n",
    "except Exception:\n",
    "    safe_print(\"FATAL: Could not obtain a batch from loaders. Ensure loaders exist and yield [B,T,C,H,W].\")\n",
    "    raise\n",
    "\n",
    "xb = xb.to(device, non_blocking=True)\n",
    "yb = yb.to(device, non_blocking=True)\n",
    "xs = xs.to(device, non_blocking=True)\n",
    "ys = ys.to(device, non_blocking=True)\n",
    "\n",
    "baseline.eval(); spherical.eval()\n",
    "\n",
    "# If running stats missing, compute from this batch only (safe and tiny)\n",
    "def compute_running_stats(y_list):\n",
    "    with torch.no_grad():\n",
    "        cat = torch.cat([F.interpolate(y, size=(Hc, Wc), mode=\"bilinear\", align_corners=False) for y in y_list], dim=0)\n",
    "        mean = cat.mean(dim=[0,2,3], keepdim=True)\n",
    "        var  = cat.var(dim=[0,2,3], unbiased=False, keepdim=True).clamp_min(1e-8)\n",
    "    return mean, var\n",
    "\n",
    "if 'running_mean' not in globals() or running_mean is None or 'running_var' not in globals() or running_var is None:\n",
    "    running_mean, running_var = compute_running_stats([yb, ys])\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward\n",
    "    yhb = baseline(xb)      # [B,C,H,W]\n",
    "    yhs = spherical(xs)     # [B,C,H,W]\n",
    "\n",
    "    # To common grid\n",
    "    yhb_c = F.interpolate(yhb, size=(Hc, Wc), mode=\"bilinear\", align_corners=False)\n",
    "    yhs_c = F.interpolate(yhs, size=(Hc, Wc), mode=\"bilinear\", align_corners=False)\n",
    "    yb_c  = F.interpolate(yb,  size=(Hc, Wc), mode=\"bilinear\", align_corners=False)\n",
    "    ys_c  = F.interpolate(ys,  size=(Hc, Wc), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # Normalize\n",
    "    def norm(y): return (y - running_mean) / torch.sqrt(running_var)\n",
    "    yhb_n, yhs_n, yb_n, ys_n = norm(yhb_c), norm(yhs_c), norm(yb_c), norm(ys_c)\n",
    "\n",
    "    # Basic metrics\n",
    "    diff_b = yhb_n - yb_n\n",
    "    diff_s = yhs_n - ys_n\n",
    "\n",
    "    mse_b = weight_reduce(diff_b.pow(2), w_lat)\n",
    "    mse_s = weight_reduce(diff_s.pow(2), w_lat)\n",
    "\n",
    "    mae_b = weight_reduce(diff_b.abs(), w_lat)\n",
    "    mae_s = weight_reduce(diff_s.abs(), w_lat)\n",
    "\n",
    "    ang_b = smooth_angle_loss(yhb_n, yb_n, w=w_lat)\n",
    "    ang_s = smooth_angle_loss(yhs_n, ys_n, w=w_lat)\n",
    "\n",
    "    # Speed magnitude error\n",
    "    spd_b = torch.sqrt((yhb_n[:,0:1]**2 + yhb_n[:,1:2]**2) + 1e-8)\n",
    "    spd_t_b = torch.sqrt((yb_n[:,0:1]**2 + yb_n[:,1:2]**2) + 1e-8)\n",
    "    mag_b = weight_reduce((spd_b - spd_t_b).abs(), w_lat)\n",
    "\n",
    "    spd_s = torch.sqrt((yhs_n[:,0:1]**2 + yhs_n[:,1:2]**2) + 1e-8)\n",
    "    spd_t_s = torch.sqrt((ys_n[:,0:1]**2 + ys_n[:,1:2]**2) + 1e-8)\n",
    "    mag_s = weight_reduce((spd_s - spd_t_s).abs(), w_lat)\n",
    "\n",
    "    # Divergence penalty proxy (smaller is better)\n",
    "    div_b = weight_reduce(div2d(yhb_n[:,0:1], yhb_n[:,1:2]).abs(), w_lat)\n",
    "    div_s = weight_reduce(div2d(yhs_n[:,0:1], yhs_n[:,1:2]).abs(), w_lat)\n",
    "\n",
    "    # Hemispheric metrics (optional, small)\n",
    "    mid = Hc // 2\n",
    "    wN = w_lat[:, :, mid:, :]\n",
    "    wS = w_lat[:, :, :mid, :]\n",
    "\n",
    "    def hemi_reduce(diff, w, hemi):\n",
    "        if hemi == \"N\":\n",
    "            d = diff[..., mid:, :]\n",
    "        else:\n",
    "            d = diff[..., :mid, :]\n",
    "        return weight_reduce(d.pow(2), w)\n",
    "\n",
    "    mse_b_N = hemi_reduce(diff_b, wN, \"N\")\n",
    "    mse_b_S = hemi_reduce(diff_b, wS, \"S\")\n",
    "    mse_s_N = hemi_reduce(diff_s, wN, \"N\")\n",
    "    mse_s_S = hemi_reduce(diff_s, wS, \"S\")\n",
    "\n",
    "    # Composite (same philosophy as before; tweak weights if desired)\n",
    "    w_mse, w_ang, w_mag, w_div = 1.0, 0.20, 0.10, 0.05\n",
    "    score_b = w_mse*mse_b + w_ang*ang_b + w_mag*mag_b + w_div*div_b\n",
    "    score_s = w_mse*mse_s + w_ang*ang_s + w_mag*mag_s + w_div*div_s\n",
    "\n",
    "# Convert to CPU floats\n",
    "def fcpu(t): return float(t.detach().cpu())\n",
    "\n",
    "eval_metrics_fast = {\n",
    "    \"baseline\": {\n",
    "        \"mse\": fcpu(mse_b), \"mae\": fcpu(mae_b), \"rmse\": float(math.sqrt(max(fcpu(mse_b), 0.0))),\n",
    "        \"angle\": fcpu(ang_b), \"mag_mae\": fcpu(mag_b), \"div_abs\": fcpu(div_b),\n",
    "        \"mse_N\": fcpu(mse_b_N), \"mse_S\": fcpu(mse_b_S),\n",
    "        \"score\": fcpu(score_b),\n",
    "    },\n",
    "    \"spherical\": {\n",
    "        \"mse\": fcpu(mse_s), \"mae\": fcpu(mae_s), \"rmse\": float(math.sqrt(max(fcpu(mse_s), 0.0))),\n",
    "        \"angle\": fcpu(ang_s), \"mag_mae\": fcpu(mag_s), \"div_abs\": fcpu(div_s),\n",
    "        \"mse_N\": fcpu(mse_s_N), \"mse_S\": fcpu(mse_s_S),\n",
    "        \"score\": fcpu(score_s),\n",
    "    },\n",
    "    \"winner\": \"spherical\" if fcpu(score_s) < fcpu(score_b) else \"baseline\",\n",
    "    \"weights\": {\"w_mse\": 1.0, \"w_ang\": 0.20, \"w_mag\": 0.10, \"w_div\": 0.05},\n",
    "    \"common_grid\": {\"H\": Hc, \"W\": Wc},\n",
    "}\n",
    "\n",
    "safe_print(\"Fast eval (Cell 12) complete.\")\n",
    "safe_print(f\"Baseline score: {eval_metrics_fast['baseline']['score']:.6f} | Spherical score: {eval_metrics_fast['spherical']['score']:.6f} | Winner: {eval_metrics_fast['winner']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38259ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warn(weights): name 'baseline' is not defined\n",
      "Saved fast_eval_metrics.json.\n",
      "Artifacts saved to: D:\\era5_runs\\spherical_vs_baseline_20250920_091042\n"
     ]
    }
   ],
   "source": [
    "# Cell 12b: Crash-proof save â€” weights + metrics JSON only (no plots)\n",
    "\n",
    "import os, json, datetime, gc\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def safe_print(msg):\n",
    "    try:\n",
    "        print(msg, flush=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Optional: clear CUDA cache to avoid transient memory pressure\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "try:\n",
    "    RUN_DIR = Path(f\"D:/era5_runs/spherical_vs_baseline_{timestamp}\")\n",
    "    RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "except Exception:\n",
    "    safe_print(\"Warn: Could not create dir on D:. Using current directory.\")\n",
    "    RUN_DIR = Path(f\"./spherical_vs_baseline_{timestamp}\")\n",
    "    RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Weights\n",
    "try:\n",
    "    torch.save({k: v.detach().cpu() for k, v in baseline.state_dict().items()}, RUN_DIR / \"baseline_weights.pt\")\n",
    "    torch.save({k: v.detach().cpu() for k, v in spherical.state_dict().items()}, RUN_DIR / \"spherical_weights.pt\")\n",
    "    safe_print(\"Saved weights.\")\n",
    "except Exception as e:\n",
    "    safe_print(f\"Warn(weights): {e}\")\n",
    "\n",
    "# Metrics\n",
    "metrics = {}\n",
    "if 'eval_metrics_fast' in globals() and isinstance(eval_metrics_fast, dict) and len(eval_metrics_fast) > 0:\n",
    "    metrics = eval_metrics_fast\n",
    "else:\n",
    "    metrics = {\"note\": \"No eval metrics found. Run Cell 12 before 12b in this session.\"}\n",
    "\n",
    "try:\n",
    "    with open(RUN_DIR / \"fast_eval_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    safe_print(\"Saved fast_eval_metrics.json.\")\n",
    "except Exception as e:\n",
    "    safe_print(f\"Warn(metrics): {e}\")\n",
    "\n",
    "# Final cleanup\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "safe_print(f\"Artifacts saved to: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f99142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from: D:\\era5_runs\\spherical_vs_baseline_20250919_214736\n"
     ]
    }
   ],
   "source": [
    "# Load previously saved weights into current models (run AFTER Cell 10, BEFORE Cell 13)\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "SAVED = Path(r\"D:\\era5_runs\\spherical_vs_baseline_20250919_214736\")\n",
    "\n",
    "# Sanity checks\n",
    "assert 'baseline' in globals() and 'spherical' in globals(), \"Models not instantiated. Run Cell 10 first.\"\n",
    "assert (SAVED / \"baseline_weights.pt\").exists(), f\"Missing: {SAVED / 'baseline_weights.pt'}\"\n",
    "assert (SAVED / \"spherical_weights.pt\").exists(), f\"Missing: {SAVED / 'spherical_weights.pt'}\"\n",
    "\n",
    "# Load to CPU (safe regardless of device); then move to whatever device you used in Cell 10 if needed\n",
    "state_b = torch.load(SAVED / \"baseline_weights.pt\", map_location=\"cpu\")\n",
    "state_s = torch.load(SAVED / \"spherical_weights.pt\", map_location=\"cpu\")\n",
    "baseline.load_state_dict(state_b)\n",
    "spherical.load_state_dict(state_s)\n",
    "\n",
    "# Ensure eval mode\n",
    "baseline.eval()\n",
    "spherical.eval()\n",
    "\n",
    "print(f\"Loaded weights from: {SAVED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e85c693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to: D:\\era5_runs\\spherical_vs_baseline_20250919_214736\\fast_metrics_report.html\n"
     ]
    }
   ],
   "source": [
    "# Resume-ready HTML report from saved metrics (no plotting, no GPU)\n",
    "# Target run folder:\n",
    "RUN_DIR = r\"D:\\era5_runs\\spherical_vs_baseline_20250919_214736\"\n",
    "\n",
    "import json, datetime, platform, sys, os\n",
    "from pathlib import Path\n",
    "from html import escape\n",
    "\n",
    "RUN = Path(RUN_DIR)\n",
    "metrics_path = RUN / \"fast_eval_metrics.json\"\n",
    "assert RUN.exists(), f\"Run folder not found: {RUN}\"\n",
    "assert metrics_path.exists(), f\"Missing metrics JSON: {metrics_path}\"\n",
    "\n",
    "# Load metrics\n",
    "with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Helpers\n",
    "def fmt_val(v):\n",
    "    if isinstance(v, float):\n",
    "        # use 6 significant digits, strip trailing zeros\n",
    "        s = f\"{v:.6g}\"\n",
    "        return s\n",
    "    return str(v)\n",
    "\n",
    "def flatten_metrics(d, prefix=\"\"):\n",
    "    rows = []\n",
    "    for k, v in d.items():\n",
    "        key = f\"{prefix}{k}\" if not prefix else f\"{prefix}.{k}\"\n",
    "        if isinstance(v, dict):\n",
    "            rows.extend(flatten_metrics(v, key))\n",
    "        else:\n",
    "            rows.append((key, fmt_val(v)))\n",
    "    return rows\n",
    "\n",
    "rows = flatten_metrics(metrics)\n",
    "\n",
    "# Attempt to pick out common fields if present\n",
    "highlights = []\n",
    "for k, v in rows:\n",
    "    kl = k.lower()\n",
    "    if any(term in kl for term in [\"mse_u\", \"mse-v\", \"mse.u\", \"mse.v\"]):\n",
    "        highlights.append((k, v))\n",
    "    if any(term in kl for term in [\"angle\", \"vector_angle\", \"dir_angle\"]):\n",
    "        highlights.append((k, v))\n",
    "    if any(term in kl for term in [\"mae\", \"rmse\"]):\n",
    "        highlights.append((k, v))\n",
    "\n",
    "# Environment info (for reproducibility)\n",
    "try:\n",
    "    import torch\n",
    "    torch_ver = torch.__version__\n",
    "    cuda_avail = torch.cuda.is_available()\n",
    "except Exception:\n",
    "    torch_ver = \"unknown\"\n",
    "    cuda_avail = False\n",
    "\n",
    "timestamp_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "sys_info = {\n",
    "    \"Generated\": timestamp_now,\n",
    "    \"OS\": f\"{platform.system()} {platform.release()}\",\n",
    "    \"Python\": sys.version.split()[0],\n",
    "    \"PyTorch\": torch_ver,\n",
    "    \"CUDA available\": str(cuda_avail),\n",
    "    \"Run folder\": str(RUN.resolve()),\n",
    "    \"Artifacts\": \", \".join([p.name for p in RUN.glob(\"*\")]),\n",
    "}\n",
    "\n",
    "# You can customize these if you wish (static descriptors for clarity)\n",
    "project_info = {\n",
    "    \"Project\": \"ERA5 500 hPa Winds â€” Spherical vs Baseline\",\n",
    "    \"Data\": \"ERA5 (500 hPa), vars: u, v\",\n",
    "    \"Grid\": \"64 x 128 (lat-lon)\",\n",
    "    \"Context Length (T)\": \"2\",\n",
    "    \"OS/Env\": \"Windows-native PyTorch (no WSL/Docker/Dask)\",\n",
    "}\n",
    "\n",
    "# Build HTML\n",
    "css = \"\"\"\n",
    "body { font-family: Arial, Helvetica, sans-serif; color: #222; padding: 24px; }\n",
    "h1, h2, h3 { margin: 0.2em 0 0.4em 0; }\n",
    "h1 { font-size: 22px; }\n",
    "h2 { font-size: 18px; color: #444; }\n",
    "h3 { font-size: 16px; color: #555; }\n",
    ".section { margin: 18px 0 22px 0; }\n",
    ".kv { display: grid; grid-template-columns: 220px 1fr; gap: 6px 14px; }\n",
    ".kv div.key { font-weight: bold; color: #333; }\n",
    "table { border-collapse: collapse; width: 100%; max-width: 900px; }\n",
    "th, td { border: 1px solid #ddd; padding: 8px 10px; }\n",
    "th { background: #f4f4f4; text-align: left; }\n",
    ".badges { display: flex; flex-wrap: wrap; gap: 8px; margin: 8px 0 0 0; }\n",
    ".badge { background: #eef3ff; color: #2a4b8d; padding: 4px 8px; border-radius: 10px; font-size: 12px; border: 1px solid #d0ddff; }\n",
    ".note { color: #666; font-size: 12px; }\n",
    "hr { border: none; height: 1px; background: #eee; margin: 18px 0; }\n",
    "\"\"\"\n",
    "\n",
    "html = []\n",
    "html.append(\"<!DOCTYPE html>\")\n",
    "html.append(\"<html><head><meta charset='utf-8'>\")\n",
    "html.append(\"<meta name='viewport' content='width=device-width, initial-scale=1' />\")\n",
    "html.append(\"<title>ERA5 Spherical vs Baseline â€” Fast Metrics Report</title>\")\n",
    "html.append(f\"<style>{css}</style>\")\n",
    "html.append(\"</head><body>\")\n",
    "\n",
    "# Title\n",
    "html.append(\"<h1>ERA5 500 hPa â€” Spherical vs Baseline</h1>\")\n",
    "html.append(\"<div class='badges'>\")\n",
    "for k, v in project_info.items():\n",
    "    html.append(f\"<span class='badge'>{escape(k)}: {escape(v)}</span>\")\n",
    "html.append(\"</div>\")\n",
    "\n",
    "# Executive summary\n",
    "html.append(\"<div class='section'>\")\n",
    "html.append(\"<h2>Executive Summary</h2>\")\n",
    "html.append(\"<p>This report summarizes fast evaluation metrics comparing a Spherical model against a Baseline model for wind prediction (u, v) on a 64Ã—128 grid with context length T=2. Models were trained and evaluated in a Windows-native PyTorch setup, avoiding WSL/Docker/Dask. The metrics below can be cited directly in a portfolio or resume.</p>\")\n",
    "html.append(\"</div>\")\n",
    "\n",
    "# Highlights (if any extracted)\n",
    "if highlights:\n",
    "    html.append(\"<div class='section'>\")\n",
    "    html.append(\"<h2>Key Highlights</h2>\")\n",
    "    html.append(\"<table><tr><th>Metric</th><th>Value</th></tr>\")\n",
    "    for k, v in highlights:\n",
    "        html.append(f\"<tr><td>{escape(k)}</td><td>{escape(v)}</td></tr>\")\n",
    "    html.append(\"</table>\")\n",
    "    html.append(\"</div>\")\n",
    "\n",
    "# Full metrics table\n",
    "html.append(\"<div class='section'>\")\n",
    "html.append(\"<h2>Full Metrics</h2>\")\n",
    "html.append(\"<table><tr><th>Metric</th><th>Value</th></tr>\")\n",
    "for k, v in rows:\n",
    "    html.append(f\"<tr><td>{escape(k)}</td><td>{escape(v)}</td></tr>\")\n",
    "html.append(\"</table>\")\n",
    "html.append(\"</div>\")\n",
    "\n",
    "# Reproducibility / Environment\n",
    "html.append(\"<div class='section'>\")\n",
    "html.append(\"<h2>Reproducibility & Environment</h2>\")\n",
    "html.append(\"<div class='kv'>\")\n",
    "for k, v in sys_info.items():\n",
    "    html.append(f\"<div class='key'>{escape(k)}</div><div>{escape(v)}</div>\")\n",
    "html.append(\"</div>\")\n",
    "html.append(\"<p class='note'>Note: This is a fast report derived solely from saved metrics (no inference, no plotting). For image panels or additional analyses, see the artifacts folder or rerun visualization cells in a stable environment.</p>\")\n",
    "html.append(\"</div>\")\n",
    "\n",
    "html.append(\"<hr>\")\n",
    "html.append(\"<div class='section note'>Generated automatically from fast_eval_metrics.json. Safe-mode report: no GPU, no plotting libraries, no dataset access.</div>\")\n",
    "html.append(\"</body></html>\")\n",
    "\n",
    "# Write HTML\n",
    "out_html = RUN / \"fast_metrics_report.html\"\n",
    "out_html.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "print(f\"Report written to: {out_html}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5097e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved one tiny batch to: D:\\era5_runs\\spherical_vs_baseline_20250919_214736\\export_for_external_viz\\one_batch_uv_64x128.npz\n",
      "Wrote manifest: D:\\era5_runs\\spherical_vs_baseline_20250919_214736\\export_for_external_viz\\export_manifest.json\n",
      "Done. You can now share the NPZ and the two weight files for external visualization.\n"
     ]
    }
   ],
   "source": [
    "# Export a single tiny batch and weights for external visualization (no inference)\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Where to write the export\n",
    "RUN_DIR = Path(r\"D:\\era5_runs\\spherical_vs_baseline_20250919_214736\")\n",
    "EXPORT_DIR = RUN_DIR / \"export_for_external_viz\"\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Get one tiny batch from val if available, else train\n",
    "def get_one_batch():\n",
    "    if 'val_b_loader' in globals() and 'val_s_loader' in globals():\n",
    "        xb, yb = next(iter(val_b_loader))\n",
    "        xs, ys = next(iter(val_s_loader))\n",
    "        split = \"val\"\n",
    "    elif 'train_b_loader' in globals() and 'train_s_loader' in globals():\n",
    "        xb, yb = next(iter(train_b_loader))\n",
    "        xs, ys = next(iter(train_s_loader))\n",
    "        split = \"train\"\n",
    "    else:\n",
    "        raise RuntimeError(\"No loaders found. Run your Cell 8 (splits + loaders) first.\")\n",
    "    # Keep only first item (B=1)\n",
    "    xb = xb[:1].detach().cpu()\n",
    "    yb = yb[:1].detach().cpu()\n",
    "    xs = xs[:1].detach().cpu()\n",
    "    return xb, yb, xs, split\n",
    "\n",
    "xb, yb, xs, split = get_one_batch()\n",
    "\n",
    "# 2) Capture minimal metadata\n",
    "meta = {\n",
    "    \"split\": split,\n",
    "    \"xb_shape\": list(xb.shape),  # [1, C, H, W] or [1, C, ...]\n",
    "    \"yb_shape\": list(yb.shape),\n",
    "    \"xs_shape\": list(xs.shape),\n",
    "    \"vars\": None,\n",
    "    \"T_ctx\": None,\n",
    "    \"grid\": {\"H\": None, \"W\": None},\n",
    "    \"notes\": [\n",
    "        \"Inputs are normalized if your Dataset applies normalization.\",\n",
    "        \"External script should use the same normalization reversal if needed.\",\n",
    "        \"Targets yb correspond to baseline target; spherical expects xs.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Try to infer VARS/T_ctx/H/W if available\n",
    "try:\n",
    "    if 'VARS' in globals():\n",
    "        meta[\"vars\"] = list(VARS)\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    if 'T_CTX' in globals():\n",
    "        meta[\"T_ctx\"] = int(T_CTX)\n",
    "except Exception:\n",
    "    pass\n",
    "# Guess H,W from xb last two dims\n",
    "if xb.ndim == 4:\n",
    "    meta[\"grid\"][\"H\"] = int(xb.shape[-2])\n",
    "    meta[\"grid\"][\"W\"] = int(xb.shape[-1])\n",
    "\n",
    "# If your datasets have a .stats attribute, export u,v mean/std snapshot\n",
    "stats_snapshot = None\n",
    "try:\n",
    "    ds_obj = val_b_loader.dataset if 'val_b_loader' in globals() else train_b_loader.dataset\n",
    "    if hasattr(ds_obj, \"stats\"):\n",
    "        # Ensure JSON serializable simple dict\n",
    "        stats_snapshot = ds_obj.stats\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 3) Save arrays as .npz\n",
    "npz_path = EXPORT_DIR / \"one_batch_uv_64x128.npz\"\n",
    "np.savez(\n",
    "    npz_path,\n",
    "    xb=xb.numpy().astype(np.float32),\n",
    "    yb=yb.numpy().astype(np.float32),\n",
    "    xs=xs.numpy().astype(np.float32),\n",
    "    meta=json.dumps(meta),\n",
    "    stats=json.dumps(stats_snapshot) if stats_snapshot is not None else json.dumps({})\n",
    ")\n",
    "print(f\"Saved one tiny batch to: {npz_path}\")\n",
    "\n",
    "# 4) Save model weights copies for convenience (no re-serialization if already .pt)\n",
    "# If you already have weight files in RUN_DIR, just copy paths below into a manifest.\n",
    "weight_manifest = {\n",
    "    \"baseline_weight_path\": None,\n",
    "    \"spherical_weight_path\": None,\n",
    "}\n",
    "\n",
    "# Try common names in the run folder\n",
    "candidates = list(RUN_DIR.glob(\"*.pt\")) + list(RUN_DIR.glob(\"**/*.pt\"))\n",
    "for p in candidates:\n",
    "    name = p.name.lower()\n",
    "    if weight_manifest[\"baseline_weight_path\"] is None and (\"baseline\" in name or \"b_\" in name):\n",
    "        weight_manifest[\"baseline_weight_path\"] = str(p.resolve())\n",
    "    if weight_manifest[\"spherical_weight_path\"] is None and (\"spherical\" in name or \"s_\" in name):\n",
    "        weight_manifest[\"spherical_weight_path\"] = str(p.resolve())\n",
    "\n",
    "# If models are in memory but no files were found, save minimal state_dicts\n",
    "if weight_manifest[\"baseline_weight_path\"] is None or weight_manifest[\"spherical_weight_path\"] is None:\n",
    "    assert 'baseline' in globals() and 'spherical' in globals(), \"Models not in memory; cannot save state_dict.\"\n",
    "    # Save minimal state_dicts to EXPORT_DIR\n",
    "    b_path = EXPORT_DIR / \"baseline_state_dict.pt\"\n",
    "    s_path = EXPORT_DIR / \"spherical_state_dict.pt\"\n",
    "    torch.save(baseline.state_dict(), b_path)\n",
    "    torch.save(spherical.state_dict(), s_path)\n",
    "    weight_manifest[\"baseline_weight_path\"] = str(b_path.resolve())\n",
    "    weight_manifest[\"spherical_weight_path\"] = str(s_path.resolve())\n",
    "\n",
    "# 5) Write a small manifest JSON for external script\n",
    "manifest = {\n",
    "    \"npz_path\": str(npz_path.resolve()),\n",
    "    \"weights\": weight_manifest,\n",
    "    \"vars\": meta[\"vars\"],\n",
    "    \"T_ctx\": meta[\"T_ctx\"],\n",
    "    \"grid\": meta[\"grid\"],\n",
    "    \"run_dir\": str(RUN_DIR.resolve()),\n",
    "}\n",
    "manifest_path = EXPORT_DIR / \"export_manifest.json\"\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(f\"Wrote manifest: {manifest_path}\")\n",
    "\n",
    "print(\"Done. You can now share the NPZ and the two weight files for external visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f293148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel_arrays.npz not found yet.\n",
      "Expected here: D:\\era5_runs\\spherical_vs_baseline_20250919_214736\\external_results\\panel_arrays.npz\n",
      "Once you have it (with keys u_t, v_t, u_b, v_b, u_s, v_s), place it there and rerun this cell.\n"
     ]
    }
   ],
   "source": [
    "# Single self-contained cell: Safe metrics + PNG visuals if/when panel_arrays.npz is present\n",
    "# - No CUDA, no matplotlib. Uses only numpy + Pillow.\n",
    "# - If the NPZ is missing, it prints instructions and exits without crashing.\n",
    "# - When the NPZ exists, it computes MSE/MAE/angle metrics and saves PNGs.\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Configure the expected NPZ path (returned from external inference)\n",
    "NPZ_PATH = Path(r\"D:\\era5_runs\\spherical_vs_baseline_20250919_214736\\external_results\\panel_arrays.npz\")\n",
    "OUT_DIR = NPZ_PATH.parent / \"png\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not NPZ_PATH.exists():\n",
    "    print(\"panel_arrays.npz not found yet.\")\n",
    "    print(f\"Expected here: {NPZ_PATH}\")\n",
    "    print(\"Once you have it (with keys u_t, v_t, u_b, v_b, u_s, v_s), place it there and rerun this cell.\")\n",
    "else:\n",
    "    # 2) Load arrays\n",
    "    d = np.load(NPZ_PATH)\n",
    "    required = [\"u_t\", \"v_t\", \"u_b\", \"v_b\", \"u_s\", \"v_s\"]\n",
    "    missing = [k for k in required if k not in d.files]\n",
    "    if missing:\n",
    "        print(\"NPZ is present but missing keys:\", missing)\n",
    "        print(\"It must contain:\", required)\n",
    "    else:\n",
    "        u_t = d[\"u_t\"].astype(np.float32); v_t = d[\"v_t\"].astype(np.float32)\n",
    "        u_b = d[\"u_b\"].astype(np.float32); v_b = d[\"v_b\"].astype(np.float32)\n",
    "        u_s = d[\"u_s\"].astype(np.float32); v_s = d[\"v_s\"].astype(np.float32)\n",
    "\n",
    "        # 3) Metrics (MSE, MAE, angle error, optional directional F1-like)\n",
    "        def mse(a, b): return float(np.nanmean((a - b) ** 2))\n",
    "        def mae(a, b): return float(np.nanmean(np.abs(a - b)))\n",
    "        def vec_angle_deg(u1, v1, u2, v2, eps=1e-8):\n",
    "            dot = u1*u2 + v1*v2\n",
    "            n1 = np.sqrt(u1*u1 + v1*v1) + eps\n",
    "            n2 = np.sqrt(u2*u2 + v2*v2) + eps\n",
    "            cos = np.clip(dot / (n1*n2), -1.0, 1.0)\n",
    "            return np.degrees(np.arccos(cos))\n",
    "\n",
    "        def directional_f1(u_hat, v_hat, u_true, v_true, theta_deg=20.0):\n",
    "            ang = vec_angle_deg(u_hat, v_hat, u_true, v_true)\n",
    "            correct = (ang <= theta_deg).astype(np.uint8)\n",
    "            mag_t = np.sqrt(u_true*u_true + v_true*v_true)\n",
    "            strong = (mag_t >= np.nanmedian(mag_t)).astype(np.uint8)\n",
    "            tp = int(np.sum((correct == 1) & (strong == 1)))\n",
    "            fp = int(np.sum((correct == 1) & (strong == 0)))\n",
    "            fn = int(np.sum((correct == 0) & (strong == 1)))\n",
    "            precision = tp / (tp + fp + 1e-9)\n",
    "            recall = tp / (tp + fn + 1e-9)\n",
    "            f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "            return {\"theta_deg\": theta_deg, \"acc\": float(np.mean(correct)),\n",
    "                    \"precision\": float(precision), \"recall\": float(recall), \"f1\": float(f1)}\n",
    "\n",
    "        metrics = {\n",
    "            \"baseline\": {\n",
    "                \"mse_u\": mse(u_b, u_t),\n",
    "                \"mse_v\": mse(v_b, v_t),\n",
    "                \"mae_u\": mae(u_b, u_t),\n",
    "                \"mae_v\": mae(v_b, v_t),\n",
    "                \"angle_deg_mean\": float(np.nanmean(vec_angle_deg(u_b, v_b, u_t, v_t))),\n",
    "                \"angle_deg_median\": float(np.nanmedian(vec_angle_deg(u_b, v_b, u_t, v_t))),\n",
    "                \"dir_f1@20deg\": directional_f1(u_b, v_b, u_t, v_t, 20.0),\n",
    "            },\n",
    "            \"spherical\": {\n",
    "                \"mse_u\": mse(u_s, u_t),\n",
    "                \"mse_v\": mse(v_s, v_t),\n",
    "                \"mae_u\": mae(u_s, u_t),\n",
    "                \"mae_v\": mae(v_s, v_t),\n",
    "                \"angle_deg_mean\": float(np.nanmean(vec_angle_deg(u_s, v_s, u_t, v_t))),\n",
    "                \"angle_deg_median\": float(np.nanmedian(vec_angle_deg(u_s, v_s, u_t, v_t))),\n",
    "                \"dir_f1@20deg\": directional_f1(u_s, v_s, u_t, v_t, 20.0),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save metrics JSON\n",
    "        metrics_path = OUT_DIR / \"panel_metrics.json\"\n",
    "        with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        print(\"Wrote metrics JSON:\", metrics_path)\n",
    "\n",
    "        # 4) Safe PNG visuals with Pillow (no matplotlib, no GPU)\n",
    "        from PIL import Image\n",
    "\n",
    "        def to_img(arr, mid=\"median\", scale=2.5):\n",
    "            a = arr.astype(np.float32).copy()\n",
    "            m = float(np.nanmedian(a)) if mid == \"median\" else float(np.nanmean(a))\n",
    "            s = float(np.nanstd(a) + 1e-6)\n",
    "            a = (a - m) / (scale * s)\n",
    "            a = np.clip(a, -1.0, 1.0)\n",
    "            r = ((a + 1.0) / 2.0) * 255.0\n",
    "            b = ((1.0 - a) / 2.0) * 255.0\n",
    "            g = 255.0 - np.abs(a) * 255.0 * 0.7\n",
    "            img = np.stack([r, g, b], axis=-1).astype(np.uint8)\n",
    "            return Image.fromarray(img)\n",
    "\n",
    "        def save_panel(img, name):\n",
    "            img = img.resize((512, 256), Image.NEAREST)\n",
    "            img.save(OUT_DIR / name)\n",
    "\n",
    "        # Fields\n",
    "        save_panel(to_img(u_t), \"truth_u.png\")\n",
    "        save_panel(to_img(v_t), \"truth_v.png\")\n",
    "        save_panel(to_img(u_b), \"baseline_u_hat.png\")\n",
    "        save_panel(to_img(v_b), \"baseline_v_hat.png\")\n",
    "        save_panel(to_img(u_s), \"spherical_u_hat.png\")\n",
    "        save_panel(to_img(v_s), \"spherical_v_hat.png\")\n",
    "\n",
    "        # Absolute error maps\n",
    "        eu_b = np.abs(u_b - u_t); ev_b = np.abs(v_b - v_t)\n",
    "        eu_s = np.abs(u_s - u_t); ev_s = np.abs(v_s - v_t)\n",
    "\n",
    "        save_panel(to_img(eu_b, mid=\"mean\", scale=4.0), \"abs_err_u_baseline.png\")\n",
    "        save_panel(to_img(ev_b, mid=\"mean\", scale=4.0), \"abs_err_v_baseline.png\")\n",
    "        save_panel(to_img(eu_s, mid=\"mean\", scale=4.0), \"abs_err_u_spherical.png\")\n",
    "        save_panel(to_img(ev_s, mid=\"mean\", scale=4.0), \"abs_err_v_spherical.png\")\n",
    "\n",
    "        print(f\"Saved PNGs to: {OUT_DIR}\")\n",
    "        print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sphericalnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
